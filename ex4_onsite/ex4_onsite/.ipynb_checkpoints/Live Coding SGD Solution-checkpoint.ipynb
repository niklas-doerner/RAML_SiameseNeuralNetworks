{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import nn_solution as nn\n",
    "import toolbox as tb\n",
    "from toolbox import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Download the following dataset: https://www.kaggle.com/brynja/wineuci\n",
    "It's the wine data again but instead of wine quality, the wine is separated into three wine types (unnamed though). \n",
    "We will train a classification network for wine types today. \n",
    "\n",
    "We will also use the implementation you did for Exercise 3. I provided the example solution but use yours if you did this exercise, its more fun! :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 14)\n",
      "(13, 178)\n",
      "(13, 149)\n",
      "(149,)\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('./archive/Wine.csv', delimiter=',')\n",
    "# first look at the data (here or in excel etc)\n",
    "# What is something you should change in this data loading code before training? \n",
    "# TODO\n",
    "np.random.shuffle(data) # shuffles along the first axis\n",
    "print(data.shape)\n",
    "\n",
    "x = data[:, 1:]\n",
    "y = data[:, 0]\n",
    "y = np.array([(int(i) - 1) for i in y])\n",
    "\n",
    "x = x / x.max(axis=0)\n",
    "x = np.transpose(x)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x_train = x[:, 1:150]\n",
    "x_val = x[:, 150:170]\n",
    "x_test = x[:, 170:]\n",
    "y_train = y[1:150]\n",
    "y_val = y[150:170]\n",
    "y_test = y[170:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "We will use a very simple classification network, a bit similar to the second lecture. But since you have all backward operations already, feel free to stack more layers and combinations when the basics work! \n",
    "\n",
    "For our training examples $(x,y), x \\in \\mathbb{R}^13, y \\in \\{ 1, 2, 3 \\}$ we want to implement the following function as a network:\n",
    "\n",
    "$$ f(x) = \\text{cross-entropy}(W x + b) $$\n",
    "\n",
    "What are the modules/layers needed here?\n",
    "\n",
    "All modules are already implemented from Exercise 3. Implement a new class ClassificationNetwork in nn.py that has the architecture hardcoded (instead of using add_layer, as we did in the last exercise session). Check back in the slides of \"Linear classification\" for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network structure\n",
    "\n",
    "network = nn.ClassificationNetwork()\n",
    "#network = nn.Network()\n",
    "#network.add_layer(nn.LinearLayer(np.array([[1., 2.], [3., 4.], [5., 6.]]), \\\n",
    "#                 np.array([[1.], [2.], [3.]])))\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_on_dataset(dataset, targets):\n",
    "    loss_value = 0\n",
    "    for i in range(len(targets)):\n",
    "        data_element = Tensor(np.expand_dims(dataset[:,i],axis=1))\n",
    "        l = loss.forward(network.forward(data_element), targets[i])\n",
    "        loss_value += l.data\n",
    "    loss_value = loss_value/len(targets)\n",
    "    return loss_value\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.78626402]\n",
      "Average training loss at initialization:  [1.16392694]\n"
     ]
    }
   ],
   "source": [
    "# run once for testing\n",
    "data = Tensor(np.expand_dims(x_train[:,0],axis=1))\n",
    "target = y_train[0]\n",
    "\n",
    "prediction = network.forward(data)\n",
    "\n",
    "l = loss.forward(prediction, target)\n",
    "print(l.data)\n",
    "\n",
    "training_loss = loss_on_dataset(x_train, y_train)\n",
    "\n",
    "print(\"Average training loss at initialization: \", training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "Next, we will train the network using our training examples. Implement Gradient Descent. If you think thats too easy, add the stochastic part and some momentum. \n",
    "\n",
    "Since we actually see the training data and actively change the output of the network to fit these, the loss on the training data is normally pretty good. That is why we monitor our training success using a validation set that is not used for updating weights. \n",
    "\n",
    "It is unlikely that you will get a loss of zero anywhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Training loss:  [0.49841909]  Validation loss:  [0.48072646]\n",
      "Iteration:  1  Training loss:  [0.49872245]  Validation loss:  [0.48051493]\n",
      "Iteration:  2  Training loss:  [0.49816563]  Validation loss:  [0.48245953]\n",
      "Iteration:  3  Training loss:  [0.49835724]  Validation loss:  [0.48244622]\n",
      "Iteration:  4  Training loss:  [0.49773522]  Validation loss:  [0.48460575]\n",
      "Iteration:  5  Training loss:  [0.49776]  Validation loss:  [0.48222953]\n",
      "Iteration:  6  Training loss:  [0.4978184]  Validation loss:  [0.48000264]\n",
      "Iteration:  7  Training loss:  [0.49731499]  Validation loss:  [0.48277411]\n",
      "Iteration:  8  Training loss:  [0.49741099]  Validation loss:  [0.48046945]\n",
      "Iteration:  9  Training loss:  [0.49714999]  Validation loss:  [0.48196879]\n",
      "Iteration:  10  Training loss:  [0.49729668]  Validation loss:  [0.47963689]\n",
      "Iteration:  11  Training loss:  [0.49720179]  Validation loss:  [0.48192142]\n",
      "Iteration:  12  Training loss:  [0.49724727]  Validation loss:  [0.48407639]\n",
      "Iteration:  13  Training loss:  [0.49742724]  Validation loss:  [0.48642696]\n",
      "Iteration:  14  Training loss:  [0.49759749]  Validation loss:  [0.48883943]\n",
      "Iteration:  15  Training loss:  [0.49683935]  Validation loss:  [0.48680266]\n",
      "Iteration:  16  Training loss:  [0.49709902]  Validation loss:  [0.48865782]\n",
      "Iteration:  17  Training loss:  [0.4972125]  Validation loss:  [0.48467894]\n",
      "Iteration:  18  Training loss:  [0.49733298]  Validation loss:  [0.4819293]\n",
      "Iteration:  19  Training loss:  [0.49679249]  Validation loss:  [0.47967999]\n",
      "Iteration:  20  Training loss:  [0.49665042]  Validation loss:  [0.47934548]\n",
      "Iteration:  21  Training loss:  [0.49658355]  Validation loss:  [0.47864229]\n",
      "Iteration:  22  Training loss:  [0.49636416]  Validation loss:  [0.48064802]\n",
      "Iteration:  23  Training loss:  [0.49631164]  Validation loss:  [0.48324769]\n",
      "Iteration:  24  Training loss:  [0.49604489]  Validation loss:  [0.48211415]\n",
      "Iteration:  25  Training loss:  [0.49620774]  Validation loss:  [0.48495902]\n",
      "Iteration:  26  Training loss:  [0.49585159]  Validation loss:  [0.48391394]\n",
      "Iteration:  27  Training loss:  [0.49578518]  Validation loss:  [0.48263966]\n",
      "Iteration:  28  Training loss:  [0.49565512]  Validation loss:  [0.48342636]\n",
      "Iteration:  29  Training loss:  [0.4957576]  Validation loss:  [0.48599871]\n",
      "Iteration:  30  Training loss:  [0.49548447]  Validation loss:  [0.48489377]\n",
      "Iteration:  31  Training loss:  [0.49552526]  Validation loss:  [0.48739353]\n",
      "Iteration:  32  Training loss:  [0.49536513]  Validation loss:  [0.48689776]\n",
      "Iteration:  33  Training loss:  [0.4954526]  Validation loss:  [0.48905562]\n",
      "Iteration:  34  Training loss:  [0.49531558]  Validation loss:  [0.4845826]\n",
      "Iteration:  35  Training loss:  [0.49543345]  Validation loss:  [0.48692497]\n",
      "Iteration:  36  Training loss:  [0.4962087]  Validation loss:  [0.48998365]\n",
      "Iteration:  37  Training loss:  [0.49535729]  Validation loss:  [0.48841598]\n",
      "Iteration:  38  Training loss:  [0.49503601]  Validation loss:  [0.48592028]\n",
      "Iteration:  39  Training loss:  [0.49503049]  Validation loss:  [0.48900826]\n",
      "Iteration:  40  Training loss:  [0.49501763]  Validation loss:  [0.4878795]\n",
      "Iteration:  41  Training loss:  [0.49521694]  Validation loss:  [0.48732348]\n",
      "Iteration:  42  Training loss:  [0.49502566]  Validation loss:  [0.49012552]\n",
      "Iteration:  43  Training loss:  [0.49499623]  Validation loss:  [0.49261914]\n",
      "Iteration:  44  Training loss:  [0.49502162]  Validation loss:  [0.49199933]\n",
      "Iteration:  45  Training loss:  [0.49506848]  Validation loss:  [0.49473918]\n",
      "Iteration:  46  Training loss:  [0.49564504]  Validation loss:  [0.49783581]\n",
      "Iteration:  47  Training loss:  [0.49525412]  Validation loss:  [0.49690569]\n",
      "Iteration:  48  Training loss:  [0.49609928]  Validation loss:  [0.50236421]\n",
      "Iteration:  49  Training loss:  [0.49703592]  Validation loss:  [0.50579892]\n",
      "Iteration:  50  Training loss:  [0.49631164]  Validation loss:  [0.50169312]\n",
      "Iteration:  51  Training loss:  [0.49709072]  Validation loss:  [0.5049993]\n",
      "Iteration:  52  Training loss:  [0.49629082]  Validation loss:  [0.50374752]\n",
      "Iteration:  53  Training loss:  [0.49558867]  Validation loss:  [0.50179993]\n",
      "Iteration:  54  Training loss:  [0.49494323]  Validation loss:  [0.49765911]\n",
      "Iteration:  55  Training loss:  [0.4943707]  Validation loss:  [0.49335086]\n",
      "Iteration:  56  Training loss:  [0.49484856]  Validation loss:  [0.49607431]\n",
      "Iteration:  57  Training loss:  [0.49451079]  Validation loss:  [0.49541018]\n",
      "Iteration:  58  Training loss:  [0.49423049]  Validation loss:  [0.49399859]\n",
      "Iteration:  59  Training loss:  [0.49430474]  Validation loss:  [0.4956431]\n",
      "Iteration:  60  Training loss:  [0.49429624]  Validation loss:  [0.4948084]\n",
      "Iteration:  61  Training loss:  [0.49445128]  Validation loss:  [0.49405526]\n",
      "Iteration:  62  Training loss:  [0.49432508]  Validation loss:  [0.49568586]\n",
      "Iteration:  63  Training loss:  [0.49443398]  Validation loss:  [0.49528041]\n",
      "Iteration:  64  Training loss:  [0.49428936]  Validation loss:  [0.49733946]\n",
      "Iteration:  65  Training loss:  [0.49436725]  Validation loss:  [0.49911384]\n",
      "Iteration:  66  Training loss:  [0.4936014]  Validation loss:  [0.4928072]\n",
      "Iteration:  67  Training loss:  [0.49395978]  Validation loss:  [0.49133528]\n",
      "Iteration:  68  Training loss:  [0.49409802]  Validation loss:  [0.49123945]\n",
      "Iteration:  69  Training loss:  [0.49352909]  Validation loss:  [0.48709105]\n",
      "Iteration:  70  Training loss:  [0.49321416]  Validation loss:  [0.48405031]\n",
      "Iteration:  71  Training loss:  [0.49351332]  Validation loss:  [0.48385339]\n",
      "Iteration:  72  Training loss:  [0.49336485]  Validation loss:  [0.48124256]\n",
      "Iteration:  73  Training loss:  [0.49331617]  Validation loss:  [0.47932726]\n",
      "Iteration:  74  Training loss:  [0.49332873]  Validation loss:  [0.4767975]\n",
      "Iteration:  75  Training loss:  [0.49401762]  Validation loss:  [0.47640638]\n",
      "Iteration:  76  Training loss:  [0.49334135]  Validation loss:  [0.47767724]\n",
      "Iteration:  77  Training loss:  [0.49372212]  Validation loss:  [0.47373539]\n",
      "Iteration:  78  Training loss:  [0.4937625]  Validation loss:  [0.47179331]\n",
      "Iteration:  79  Training loss:  [0.49417064]  Validation loss:  [0.47166559]\n",
      "Iteration:  80  Training loss:  [0.49602506]  Validation loss:  [0.47182225]\n",
      "Iteration:  81  Training loss:  [0.49476518]  Validation loss:  [0.47257451]\n",
      "Iteration:  82  Training loss:  [0.49336823]  Validation loss:  [0.47403358]\n",
      "Iteration:  83  Training loss:  [0.49264169]  Validation loss:  [0.47639764]\n",
      "Iteration:  84  Training loss:  [0.49205695]  Validation loss:  [0.47816163]\n",
      "Iteration:  85  Training loss:  [0.49173546]  Validation loss:  [0.47952378]\n",
      "Iteration:  86  Training loss:  [0.49172239]  Validation loss:  [0.47709995]\n",
      "Iteration:  87  Training loss:  [0.49200286]  Validation loss:  [0.47625287]\n",
      "Iteration:  88  Training loss:  [0.49205657]  Validation loss:  [0.47395498]\n",
      "Iteration:  89  Training loss:  [0.49153923]  Validation loss:  [0.47734055]\n",
      "Iteration:  90  Training loss:  [0.49131533]  Validation loss:  [0.47929988]\n",
      "Iteration:  91  Training loss:  [0.49129484]  Validation loss:  [0.48210416]\n",
      "Iteration:  92  Training loss:  [0.49126257]  Validation loss:  [0.48062795]\n",
      "Iteration:  93  Training loss:  [0.49132882]  Validation loss:  [0.4803848]\n",
      "Iteration:  94  Training loss:  [0.49107118]  Validation loss:  [0.48242307]\n",
      "Iteration:  95  Training loss:  [0.49097475]  Validation loss:  [0.48349675]\n",
      "Iteration:  96  Training loss:  [0.49101017]  Validation loss:  [0.48300933]\n",
      "Iteration:  97  Training loss:  [0.49103213]  Validation loss:  [0.48673082]\n",
      "Iteration:  98  Training loss:  [0.49080015]  Validation loss:  [0.48370108]\n",
      "Iteration:  99  Training loss:  [0.49087799]  Validation loss:  [0.48340227]\n",
      "Iteration:  100  Training loss:  [0.4910173]  Validation loss:  [0.48305029]\n",
      "Iteration:  101  Training loss:  [0.49092368]  Validation loss:  [0.47879011]\n",
      "Iteration:  102  Training loss:  [0.49062328]  Validation loss:  [0.47972667]\n",
      "Iteration:  103  Training loss:  [0.4904869]  Validation loss:  [0.48338381]\n",
      "Iteration:  104  Training loss:  [0.49074739]  Validation loss:  [0.48300504]\n",
      "Iteration:  105  Training loss:  [0.49046892]  Validation loss:  [0.48526226]\n",
      "Iteration:  106  Training loss:  [0.49027817]  Validation loss:  [0.48068138]\n",
      "Iteration:  107  Training loss:  [0.49017689]  Validation loss:  [0.48261482]\n",
      "Iteration:  108  Training loss:  [0.49055315]  Validation loss:  [0.48195307]\n",
      "Iteration:  109  Training loss:  [0.49038376]  Validation loss:  [0.4793334]\n",
      "Iteration:  110  Training loss:  [0.49000061]  Validation loss:  [0.48150564]\n",
      "Iteration:  111  Training loss:  [0.4900732]  Validation loss:  [0.48371712]\n",
      "Iteration:  112  Training loss:  [0.48999613]  Validation loss:  [0.48186282]\n",
      "Iteration:  113  Training loss:  [0.49008624]  Validation loss:  [0.48139916]\n",
      "Iteration:  114  Training loss:  [0.48990142]  Validation loss:  [0.47909659]\n",
      "Iteration:  115  Training loss:  [0.4904908]  Validation loss:  [0.47855555]\n",
      "Iteration:  116  Training loss:  [0.49037372]  Validation loss:  [0.47568114]\n",
      "Iteration:  117  Training loss:  [0.49054924]  Validation loss:  [0.47564601]\n",
      "Iteration:  118  Training loss:  [0.48980672]  Validation loss:  [0.47726567]\n",
      "Iteration:  119  Training loss:  [0.49018077]  Validation loss:  [0.47676807]\n",
      "Iteration:  120  Training loss:  [0.48975836]  Validation loss:  [0.47935495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  121  Training loss:  [0.48952498]  Validation loss:  [0.48206274]\n",
      "Iteration:  122  Training loss:  [0.48946554]  Validation loss:  [0.47767316]\n",
      "Iteration:  123  Training loss:  [0.48919291]  Validation loss:  [0.48031934]\n",
      "Iteration:  124  Training loss:  [0.48905105]  Validation loss:  [0.48194333]\n",
      "Iteration:  125  Training loss:  [0.48903454]  Validation loss:  [0.48344612]\n",
      "Iteration:  126  Training loss:  [0.48895103]  Validation loss:  [0.48304854]\n",
      "Iteration:  127  Training loss:  [0.48916575]  Validation loss:  [0.48565497]\n",
      "Iteration:  128  Training loss:  [0.48894328]  Validation loss:  [0.48322948]\n",
      "Iteration:  129  Training loss:  [0.48877112]  Validation loss:  [0.48024011]\n",
      "Iteration:  130  Training loss:  [0.48867959]  Validation loss:  [0.4776106]\n",
      "Iteration:  131  Training loss:  [0.48841413]  Validation loss:  [0.47615741]\n",
      "Iteration:  132  Training loss:  [0.48840273]  Validation loss:  [0.47389268]\n",
      "Iteration:  133  Training loss:  [0.48855444]  Validation loss:  [0.47092706]\n",
      "Iteration:  134  Training loss:  [0.48878407]  Validation loss:  [0.47351754]\n",
      "Iteration:  135  Training loss:  [0.48892447]  Validation loss:  [0.47527857]\n",
      "Iteration:  136  Training loss:  [0.48834183]  Validation loss:  [0.47386063]\n",
      "Iteration:  137  Training loss:  [0.48843657]  Validation loss:  [0.47172569]\n",
      "Iteration:  138  Training loss:  [0.48849597]  Validation loss:  [0.47287417]\n",
      "Iteration:  139  Training loss:  [0.48869869]  Validation loss:  [0.47779987]\n",
      "Iteration:  140  Training loss:  [0.48916671]  Validation loss:  [0.48025462]\n",
      "Iteration:  141  Training loss:  [0.48807933]  Validation loss:  [0.47748177]\n",
      "Iteration:  142  Training loss:  [0.48803758]  Validation loss:  [0.47523077]\n",
      "Iteration:  143  Training loss:  [0.4881069]  Validation loss:  [0.47175218]\n",
      "Iteration:  144  Training loss:  [0.48784111]  Validation loss:  [0.47117751]\n",
      "Iteration:  145  Training loss:  [0.48790739]  Validation loss:  [0.47525303]\n",
      "Iteration:  146  Training loss:  [0.48824254]  Validation loss:  [0.47712732]\n",
      "Iteration:  147  Training loss:  [0.48763175]  Validation loss:  [0.47566665]\n",
      "Iteration:  148  Training loss:  [0.48727671]  Validation loss:  [0.47420695]\n",
      "Iteration:  149  Training loss:  [0.48714964]  Validation loss:  [0.47369875]\n",
      "Iteration:  150  Training loss:  [0.4870554]  Validation loss:  [0.47445105]\n",
      "Iteration:  151  Training loss:  [0.48726581]  Validation loss:  [0.47705601]\n",
      "Iteration:  152  Training loss:  [0.48769984]  Validation loss:  [0.48215054]\n",
      "Iteration:  153  Training loss:  [0.48798945]  Validation loss:  [0.48411879]\n",
      "Iteration:  154  Training loss:  [0.4889587]  Validation loss:  [0.4875076]\n",
      "Iteration:  155  Training loss:  [0.48981457]  Validation loss:  [0.49061229]\n",
      "Iteration:  156  Training loss:  [0.48963255]  Validation loss:  [0.48868854]\n",
      "Iteration:  157  Training loss:  [0.488333]  Validation loss:  [0.4861815]\n",
      "Iteration:  158  Training loss:  [0.48883234]  Validation loss:  [0.48778275]\n",
      "Iteration:  159  Training loss:  [0.48840196]  Validation loss:  [0.48393092]\n",
      "Iteration:  160  Training loss:  [0.48885038]  Validation loss:  [0.48532635]\n",
      "Iteration:  161  Training loss:  [0.48965509]  Validation loss:  [0.48789238]\n",
      "Iteration:  162  Training loss:  [0.48824601]  Validation loss:  [0.48538829]\n",
      "Iteration:  163  Training loss:  [0.48898534]  Validation loss:  [0.48830019]\n",
      "Iteration:  164  Training loss:  [0.49029596]  Validation loss:  [0.49134725]\n",
      "Iteration:  165  Training loss:  [0.49127624]  Validation loss:  [0.49482143]\n",
      "Iteration:  166  Training loss:  [0.49241403]  Validation loss:  [0.49759976]\n",
      "Iteration:  167  Training loss:  [0.49081092]  Validation loss:  [0.49549606]\n",
      "Iteration:  168  Training loss:  [0.49195303]  Validation loss:  [0.4979621]\n",
      "Iteration:  169  Training loss:  [0.49118619]  Validation loss:  [0.49398693]\n",
      "Iteration:  170  Training loss:  [0.49266267]  Validation loss:  [0.4987122]\n",
      "Iteration:  171  Training loss:  [0.49381368]  Validation loss:  [0.50096923]\n",
      "Iteration:  172  Training loss:  [0.49576608]  Validation loss:  [0.50458669]\n",
      "Iteration:  173  Training loss:  [0.49432552]  Validation loss:  [0.49936156]\n",
      "Iteration:  174  Training loss:  [0.49622406]  Validation loss:  [0.50444498]\n",
      "Iteration:  175  Training loss:  [0.49391465]  Validation loss:  [0.50176867]\n",
      "Iteration:  176  Training loss:  [0.49243604]  Validation loss:  [0.49981102]\n",
      "Iteration:  177  Training loss:  [0.48974202]  Validation loss:  [0.49544772]\n",
      "Iteration:  178  Training loss:  [0.48880382]  Validation loss:  [0.49402256]\n",
      "Iteration:  179  Training loss:  [0.49008105]  Validation loss:  [0.49707328]\n",
      "Iteration:  180  Training loss:  [0.48949877]  Validation loss:  [0.49377031]\n",
      "Iteration:  181  Training loss:  [0.4903157]  Validation loss:  [0.49597372]\n",
      "Iteration:  182  Training loss:  [0.48909088]  Validation loss:  [0.48994446]\n",
      "Iteration:  183  Training loss:  [0.48887688]  Validation loss:  [0.48631132]\n",
      "Iteration:  184  Training loss:  [0.48755664]  Validation loss:  [0.48422333]\n",
      "Iteration:  185  Training loss:  [0.48649276]  Validation loss:  [0.4826746]\n",
      "Iteration:  186  Training loss:  [0.48597825]  Validation loss:  [0.47800241]\n",
      "Iteration:  187  Training loss:  [0.48619957]  Validation loss:  [0.47892627]\n",
      "Iteration:  188  Training loss:  [0.4850233]  Validation loss:  [0.47638433]\n",
      "Iteration:  189  Training loss:  [0.48515532]  Validation loss:  [0.47725535]\n",
      "Iteration:  190  Training loss:  [0.48508367]  Validation loss:  [0.47513605]\n",
      "Iteration:  191  Training loss:  [0.48459801]  Validation loss:  [0.47420597]\n",
      "Iteration:  192  Training loss:  [0.48488529]  Validation loss:  [0.47598154]\n",
      "Iteration:  193  Training loss:  [0.48533524]  Validation loss:  [0.47845609]\n",
      "Iteration:  194  Training loss:  [0.48576838]  Validation loss:  [0.48045294]\n",
      "Iteration:  195  Training loss:  [0.48537454]  Validation loss:  [0.47984262]\n",
      "Iteration:  196  Training loss:  [0.4847958]  Validation loss:  [0.47880351]\n",
      "Iteration:  197  Training loss:  [0.48517843]  Validation loss:  [0.48031445]\n",
      "Iteration:  198  Training loss:  [0.4850357]  Validation loss:  [0.47785231]\n",
      "Iteration:  199  Training loss:  [0.48427568]  Validation loss:  [0.47672998]\n",
      "Iteration:  200  Training loss:  [0.48418662]  Validation loss:  [0.47436754]\n",
      "Iteration:  201  Training loss:  [0.48476029]  Validation loss:  [0.47666629]\n",
      "Iteration:  202  Training loss:  [0.48392868]  Validation loss:  [0.47504521]\n",
      "Iteration:  203  Training loss:  [0.48359145]  Validation loss:  [0.47438969]\n",
      "Iteration:  204  Training loss:  [0.48411827]  Validation loss:  [0.47667262]\n",
      "Iteration:  205  Training loss:  [0.48442611]  Validation loss:  [0.47838299]\n",
      "Iteration:  206  Training loss:  [0.48431606]  Validation loss:  [0.47598601]\n",
      "Iteration:  207  Training loss:  [0.48475049]  Validation loss:  [0.47791421]\n",
      "Iteration:  208  Training loss:  [0.48457983]  Validation loss:  [0.47522919]\n",
      "Iteration:  209  Training loss:  [0.48549247]  Validation loss:  [0.47799066]\n",
      "Iteration:  210  Training loss:  [0.4839271]  Validation loss:  [0.47509663]\n",
      "Iteration:  211  Training loss:  [0.48444081]  Validation loss:  [0.47692639]\n",
      "Iteration:  212  Training loss:  [0.48353932]  Validation loss:  [0.47558401]\n",
      "Iteration:  213  Training loss:  [0.48361307]  Validation loss:  [0.47231247]\n",
      "Iteration:  214  Training loss:  [0.48267217]  Validation loss:  [0.46929146]\n",
      "Iteration:  215  Training loss:  [0.48285907]  Validation loss:  [0.47149702]\n",
      "Iteration:  216  Training loss:  [0.48283886]  Validation loss:  [0.46954901]\n",
      "Iteration:  217  Training loss:  [0.48301367]  Validation loss:  [0.47246566]\n",
      "Iteration:  218  Training loss:  [0.48248679]  Validation loss:  [0.47109002]\n",
      "Iteration:  219  Training loss:  [0.48224267]  Validation loss:  [0.47028468]\n",
      "Iteration:  220  Training loss:  [0.48232186]  Validation loss:  [0.4715955]\n",
      "Iteration:  221  Training loss:  [0.48226166]  Validation loss:  [0.46858133]\n",
      "Iteration:  222  Training loss:  [0.48235593]  Validation loss:  [0.46971583]\n",
      "Iteration:  223  Training loss:  [0.48312149]  Validation loss:  [0.47265633]\n",
      "Iteration:  224  Training loss:  [0.48295908]  Validation loss:  [0.46857962]\n",
      "Iteration:  225  Training loss:  [0.48236214]  Validation loss:  [0.46716301]\n",
      "Iteration:  226  Training loss:  [0.4824477]  Validation loss:  [0.46937584]\n",
      "Iteration:  227  Training loss:  [0.48264876]  Validation loss:  [0.47100814]\n",
      "Iteration:  228  Training loss:  [0.48342237]  Validation loss:  [0.47377127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  229  Training loss:  [0.48321023]  Validation loss:  [0.47103888]\n",
      "Iteration:  230  Training loss:  [0.48219221]  Validation loss:  [0.46902153]\n",
      "Iteration:  231  Training loss:  [0.48213531]  Validation loss:  [0.46651326]\n",
      "Iteration:  232  Training loss:  [0.48213282]  Validation loss:  [0.4648134]\n",
      "Iteration:  233  Training loss:  [0.48235295]  Validation loss:  [0.46118032]\n",
      "Iteration:  234  Training loss:  [0.48241313]  Validation loss:  [0.4628464]\n",
      "Iteration:  235  Training loss:  [0.48309147]  Validation loss:  [0.46590284]\n",
      "Iteration:  236  Training loss:  [0.48243405]  Validation loss:  [0.46466384]\n",
      "Iteration:  237  Training loss:  [0.48266185]  Validation loss:  [0.46649694]\n",
      "Iteration:  238  Training loss:  [0.48347438]  Validation loss:  [0.46927061]\n",
      "Iteration:  239  Training loss:  [0.48479306]  Validation loss:  [0.47276835]\n",
      "Iteration:  240  Training loss:  [0.48390608]  Validation loss:  [0.47160374]\n",
      "Iteration:  241  Training loss:  [0.48245195]  Validation loss:  [0.46906507]\n",
      "Iteration:  242  Training loss:  [0.4817012]  Validation loss:  [0.46773649]\n",
      "Iteration:  243  Training loss:  [0.48108486]  Validation loss:  [0.4663031]\n",
      "Iteration:  244  Training loss:  [0.48120019]  Validation loss:  [0.46336441]\n",
      "Iteration:  245  Training loss:  [0.48079273]  Validation loss:  [0.46173559]\n",
      "Iteration:  246  Training loss:  [0.48069986]  Validation loss:  [0.46446484]\n",
      "Iteration:  247  Training loss:  [0.48035994]  Validation loss:  [0.46347688]\n",
      "Iteration:  248  Training loss:  [0.48031508]  Validation loss:  [0.46283092]\n",
      "Iteration:  249  Training loss:  [0.48011298]  Validation loss:  [0.46634069]\n",
      "Iteration:  250  Training loss:  [0.48009153]  Validation loss:  [0.46800378]\n",
      "Iteration:  251  Training loss:  [0.48031295]  Validation loss:  [0.4701767]\n",
      "Iteration:  252  Training loss:  [0.48116179]  Validation loss:  [0.47336296]\n",
      "Iteration:  253  Training loss:  [0.48177074]  Validation loss:  [0.47536657]\n",
      "Iteration:  254  Training loss:  [0.4825374]  Validation loss:  [0.4778329]\n",
      "Iteration:  255  Training loss:  [0.48225523]  Validation loss:  [0.4734814]\n",
      "Iteration:  256  Training loss:  [0.48148705]  Validation loss:  [0.47251773]\n",
      "Iteration:  257  Training loss:  [0.48249788]  Validation loss:  [0.47509168]\n",
      "Iteration:  258  Training loss:  [0.48227528]  Validation loss:  [0.47191755]\n",
      "Iteration:  259  Training loss:  [0.4821525]  Validation loss:  [0.46785478]\n",
      "Iteration:  260  Training loss:  [0.48223517]  Validation loss:  [0.46568992]\n",
      "Iteration:  261  Training loss:  [0.48258305]  Validation loss:  [0.46741313]\n",
      "Iteration:  262  Training loss:  [0.48327185]  Validation loss:  [0.46931221]\n",
      "Iteration:  263  Training loss:  [0.48397568]  Validation loss:  [0.47173792]\n",
      "Iteration:  264  Training loss:  [0.48477416]  Validation loss:  [0.47499908]\n",
      "Iteration:  265  Training loss:  [0.48543429]  Validation loss:  [0.47684444]\n",
      "Iteration:  266  Training loss:  [0.48728026]  Validation loss:  [0.48050473]\n",
      "Iteration:  267  Training loss:  [0.48422678]  Validation loss:  [0.47672608]\n",
      "Iteration:  268  Training loss:  [0.4823265]  Validation loss:  [0.47369608]\n",
      "Iteration:  269  Training loss:  [0.48287385]  Validation loss:  [0.47546984]\n",
      "Iteration:  270  Training loss:  [0.48357728]  Validation loss:  [0.47759478]\n",
      "Iteration:  271  Training loss:  [0.48282654]  Validation loss:  [0.47667261]\n",
      "Iteration:  272  Training loss:  [0.48174727]  Validation loss:  [0.47510257]\n",
      "Iteration:  273  Training loss:  [0.48028237]  Validation loss:  [0.47265237]\n",
      "Iteration:  274  Training loss:  [0.47991626]  Validation loss:  [0.46797691]\n",
      "Iteration:  275  Training loss:  [0.48087832]  Validation loss:  [0.470908]\n",
      "Iteration:  276  Training loss:  [0.48215373]  Validation loss:  [0.47442566]\n",
      "Iteration:  277  Training loss:  [0.4805857]  Validation loss:  [0.47171284]\n",
      "Iteration:  278  Training loss:  [0.48150793]  Validation loss:  [0.4760222]\n",
      "Iteration:  279  Training loss:  [0.48129894]  Validation loss:  [0.47336058]\n",
      "Iteration:  280  Training loss:  [0.48101463]  Validation loss:  [0.46907397]\n",
      "Iteration:  281  Training loss:  [0.48080643]  Validation loss:  [0.46507241]\n",
      "Iteration:  282  Training loss:  [0.48089947]  Validation loss:  [0.46243748]\n",
      "Iteration:  283  Training loss:  [0.47988212]  Validation loss:  [0.46098202]\n",
      "Iteration:  284  Training loss:  [0.48015869]  Validation loss:  [0.46282037]\n",
      "Iteration:  285  Training loss:  [0.47905676]  Validation loss:  [0.46075775]\n",
      "Iteration:  286  Training loss:  [0.47869712]  Validation loss:  [0.46010705]\n",
      "Iteration:  287  Training loss:  [0.4781577]  Validation loss:  [0.45790649]\n",
      "Iteration:  288  Training loss:  [0.47835271]  Validation loss:  [0.45590615]\n",
      "Iteration:  289  Training loss:  [0.47888835]  Validation loss:  [0.45278183]\n",
      "Iteration:  290  Training loss:  [0.47849053]  Validation loss:  [0.45526732]\n",
      "Iteration:  291  Training loss:  [0.47831847]  Validation loss:  [0.45480247]\n",
      "Iteration:  292  Training loss:  [0.47825948]  Validation loss:  [0.4558024]\n",
      "Iteration:  293  Training loss:  [0.47842223]  Validation loss:  [0.45792815]\n",
      "Iteration:  294  Training loss:  [0.47893048]  Validation loss:  [0.46051312]\n",
      "Iteration:  295  Training loss:  [0.47858383]  Validation loss:  [0.45998084]\n",
      "Iteration:  296  Training loss:  [0.47770507]  Validation loss:  [0.45827839]\n",
      "Iteration:  297  Training loss:  [0.47737586]  Validation loss:  [0.45767494]\n",
      "Iteration:  298  Training loss:  [0.4775961]  Validation loss:  [0.4556623]\n",
      "Iteration:  299  Training loss:  [0.47787212]  Validation loss:  [0.4538386]\n",
      "Iteration:  300  Training loss:  [0.47739335]  Validation loss:  [0.45563314]\n",
      "Iteration:  301  Training loss:  [0.47754947]  Validation loss:  [0.45386677]\n",
      "Iteration:  302  Training loss:  [0.47769067]  Validation loss:  [0.45350482]\n",
      "Iteration:  303  Training loss:  [0.47724954]  Validation loss:  [0.45482441]\n",
      "Iteration:  304  Training loss:  [0.47732591]  Validation loss:  [0.45468785]\n",
      "Iteration:  305  Training loss:  [0.47783758]  Validation loss:  [0.45149829]\n",
      "Iteration:  306  Training loss:  [0.47825841]  Validation loss:  [0.44934341]\n",
      "Iteration:  307  Training loss:  [0.47863451]  Validation loss:  [0.44768068]\n",
      "Iteration:  308  Training loss:  [0.4792955]  Validation loss:  [0.44735913]\n",
      "Iteration:  309  Training loss:  [0.47954203]  Validation loss:  [0.44730171]\n",
      "Iteration:  310  Training loss:  [0.47981684]  Validation loss:  [0.44592775]\n",
      "Iteration:  311  Training loss:  [0.48041764]  Validation loss:  [0.44622284]\n",
      "Iteration:  312  Training loss:  [0.48075768]  Validation loss:  [0.44482375]\n",
      "Iteration:  313  Training loss:  [0.48135296]  Validation loss:  [0.44500845]\n",
      "Iteration:  314  Training loss:  [0.4798761]  Validation loss:  [0.44593314]\n",
      "Iteration:  315  Training loss:  [0.48047322]  Validation loss:  [0.44594653]\n",
      "Iteration:  316  Training loss:  [0.48109591]  Validation loss:  [0.44613533]\n",
      "Iteration:  317  Training loss:  [0.48148552]  Validation loss:  [0.44437392]\n",
      "Iteration:  318  Training loss:  [0.48013875]  Validation loss:  [0.44498004]\n",
      "Iteration:  319  Training loss:  [0.48126705]  Validation loss:  [0.44536096]\n",
      "Iteration:  320  Training loss:  [0.47938895]  Validation loss:  [0.44609028]\n",
      "Iteration:  321  Training loss:  [0.47794663]  Validation loss:  [0.44860273]\n",
      "Iteration:  322  Training loss:  [0.47703922]  Validation loss:  [0.44993073]\n",
      "Iteration:  323  Training loss:  [0.47600616]  Validation loss:  [0.45306323]\n",
      "Iteration:  324  Training loss:  [0.47554685]  Validation loss:  [0.45489389]\n",
      "Iteration:  325  Training loss:  [0.47559251]  Validation loss:  [0.4547405]\n",
      "Iteration:  326  Training loss:  [0.47516052]  Validation loss:  [0.45651397]\n",
      "Iteration:  327  Training loss:  [0.47543621]  Validation loss:  [0.45575894]\n",
      "Iteration:  328  Training loss:  [0.4754932]  Validation loss:  [0.45425879]\n",
      "Iteration:  329  Training loss:  [0.47573655]  Validation loss:  [0.45189552]\n",
      "Iteration:  330  Training loss:  [0.47589023]  Validation loss:  [0.45010924]\n",
      "Iteration:  331  Training loss:  [0.47622625]  Validation loss:  [0.44770529]\n",
      "Iteration:  332  Training loss:  [0.47683938]  Validation loss:  [0.44731924]\n",
      "Iteration:  333  Training loss:  [0.47602621]  Validation loss:  [0.44887554]\n",
      "Iteration:  334  Training loss:  [0.47539905]  Validation loss:  [0.45004696]\n",
      "Iteration:  335  Training loss:  [0.47491681]  Validation loss:  [0.45183457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  336  Training loss:  [0.47461339]  Validation loss:  [0.45350396]\n",
      "Iteration:  337  Training loss:  [0.47445652]  Validation loss:  [0.45499384]\n",
      "Iteration:  338  Training loss:  [0.47435486]  Validation loss:  [0.45418881]\n",
      "Iteration:  339  Training loss:  [0.47478661]  Validation loss:  [0.4536643]\n",
      "Iteration:  340  Training loss:  [0.47572816]  Validation loss:  [0.45360317]\n",
      "Iteration:  341  Training loss:  [0.47637677]  Validation loss:  [0.453427]\n",
      "Iteration:  342  Training loss:  [0.47634791]  Validation loss:  [0.45164465]\n",
      "Iteration:  343  Training loss:  [0.47741545]  Validation loss:  [0.45199543]\n",
      "Iteration:  344  Training loss:  [0.47783578]  Validation loss:  [0.45209707]\n",
      "Iteration:  345  Training loss:  [0.47791172]  Validation loss:  [0.44981362]\n",
      "Iteration:  346  Training loss:  [0.47816349]  Validation loss:  [0.44763942]\n",
      "Iteration:  347  Training loss:  [0.47690045]  Validation loss:  [0.44876641]\n",
      "Iteration:  348  Training loss:  [0.47819966]  Validation loss:  [0.44885287]\n",
      "Iteration:  349  Training loss:  [0.47903155]  Validation loss:  [0.44587285]\n",
      "Iteration:  350  Training loss:  [0.47760734]  Validation loss:  [0.44684202]\n",
      "Iteration:  351  Training loss:  [0.47630496]  Validation loss:  [0.44789766]\n",
      "Iteration:  352  Training loss:  [0.47713828]  Validation loss:  [0.44511264]\n",
      "Iteration:  353  Training loss:  [0.47575654]  Validation loss:  [0.44613162]\n",
      "Iteration:  354  Training loss:  [0.47619082]  Validation loss:  [0.44446019]\n",
      "Iteration:  355  Training loss:  [0.47804372]  Validation loss:  [0.44466156]\n",
      "Iteration:  356  Training loss:  [0.47919465]  Validation loss:  [0.44499552]\n",
      "Iteration:  357  Training loss:  [0.47730802]  Validation loss:  [0.44563454]\n",
      "Iteration:  358  Training loss:  [0.47754024]  Validation loss:  [0.4440895]\n",
      "Iteration:  359  Training loss:  [0.47614363]  Validation loss:  [0.44585762]\n",
      "Iteration:  360  Training loss:  [0.4767791]  Validation loss:  [0.44386394]\n",
      "Iteration:  361  Training loss:  [0.477]  Validation loss:  [0.44245646]\n",
      "Iteration:  362  Training loss:  [0.47726158]  Validation loss:  [0.44112404]\n",
      "Iteration:  363  Training loss:  [0.47765491]  Validation loss:  [0.43969666]\n",
      "Iteration:  364  Training loss:  [0.4761901]  Validation loss:  [0.44093616]\n",
      "Iteration:  365  Training loss:  [0.47681592]  Validation loss:  [0.44078861]\n",
      "Iteration:  366  Training loss:  [0.4773553]  Validation loss:  [0.44090158]\n",
      "Iteration:  367  Training loss:  [0.47800774]  Validation loss:  [0.44124768]\n",
      "Iteration:  368  Training loss:  [0.4782733]  Validation loss:  [0.44008868]\n",
      "Iteration:  369  Training loss:  [0.47889476]  Validation loss:  [0.43875608]\n",
      "Iteration:  370  Training loss:  [0.47716873]  Validation loss:  [0.43992732]\n",
      "Iteration:  371  Training loss:  [0.47796255]  Validation loss:  [0.43816823]\n",
      "Iteration:  372  Training loss:  [0.47855365]  Validation loss:  [0.43818926]\n",
      "Iteration:  373  Training loss:  [0.47694696]  Validation loss:  [0.43911364]\n",
      "Iteration:  374  Training loss:  [0.47766819]  Validation loss:  [0.43932987]\n",
      "Iteration:  375  Training loss:  [0.4761018]  Validation loss:  [0.44059028]\n",
      "Iteration:  376  Training loss:  [0.47448869]  Validation loss:  [0.44300059]\n",
      "Iteration:  377  Training loss:  [0.47513611]  Validation loss:  [0.44108451]\n",
      "Iteration:  378  Training loss:  [0.47439443]  Validation loss:  [0.4416051]\n",
      "Iteration:  379  Training loss:  [0.47502772]  Validation loss:  [0.43956966]\n",
      "Iteration:  380  Training loss:  [0.47398788]  Validation loss:  [0.44078855]\n",
      "Iteration:  381  Training loss:  [0.47326385]  Validation loss:  [0.44199445]\n",
      "Iteration:  382  Training loss:  [0.47380031]  Validation loss:  [0.44017138]\n",
      "Iteration:  383  Training loss:  [0.47324862]  Validation loss:  [0.44101136]\n",
      "Iteration:  384  Training loss:  [0.47266943]  Validation loss:  [0.44306507]\n",
      "Iteration:  385  Training loss:  [0.47247341]  Validation loss:  [0.44412423]\n",
      "Iteration:  386  Training loss:  [0.47221246]  Validation loss:  [0.44637841]\n",
      "Iteration:  387  Training loss:  [0.4721445]  Validation loss:  [0.44770267]\n",
      "Iteration:  388  Training loss:  [0.47176728]  Validation loss:  [0.44613961]\n",
      "Iteration:  389  Training loss:  [0.47151519]  Validation loss:  [0.44805492]\n",
      "Iteration:  390  Training loss:  [0.47202683]  Validation loss:  [0.44576151]\n",
      "Iteration:  391  Training loss:  [0.47182704]  Validation loss:  [0.44770606]\n",
      "Iteration:  392  Training loss:  [0.47172863]  Validation loss:  [0.45224928]\n",
      "Iteration:  393  Training loss:  [0.47191237]  Validation loss:  [0.44982649]\n",
      "Iteration:  394  Training loss:  [0.47214237]  Validation loss:  [0.44810774]\n",
      "Iteration:  395  Training loss:  [0.47148614]  Validation loss:  [0.44673916]\n",
      "Iteration:  396  Training loss:  [0.47117183]  Validation loss:  [0.4462015]\n",
      "Iteration:  397  Training loss:  [0.47107584]  Validation loss:  [0.4476867]\n",
      "Iteration:  398  Training loss:  [0.47067682]  Validation loss:  [0.44687581]\n",
      "Iteration:  399  Training loss:  [0.47046908]  Validation loss:  [0.44880462]\n",
      "Iteration:  400  Training loss:  [0.470274]  Validation loss:  [0.44783846]\n",
      "Iteration:  401  Training loss:  [0.47034044]  Validation loss:  [0.45005161]\n",
      "Iteration:  402  Training loss:  [0.4704229]  Validation loss:  [0.45159242]\n",
      "Iteration:  403  Training loss:  [0.47049603]  Validation loss:  [0.44887124]\n",
      "Iteration:  404  Training loss:  [0.47001781]  Validation loss:  [0.44743451]\n",
      "Iteration:  405  Training loss:  [0.47026686]  Validation loss:  [0.44543052]\n",
      "Iteration:  406  Training loss:  [0.47038376]  Validation loss:  [0.44418866]\n",
      "Iteration:  407  Training loss:  [0.47051964]  Validation loss:  [0.44383457]\n",
      "Iteration:  408  Training loss:  [0.47117048]  Validation loss:  [0.44343175]\n",
      "Iteration:  409  Training loss:  [0.47051838]  Validation loss:  [0.44455328]\n",
      "Iteration:  410  Training loss:  [0.47082981]  Validation loss:  [0.44428745]\n",
      "Iteration:  411  Training loss:  [0.47013867]  Validation loss:  [0.44528674]\n",
      "Iteration:  412  Training loss:  [0.46965307]  Validation loss:  [0.44650993]\n",
      "Iteration:  413  Training loss:  [0.46925061]  Validation loss:  [0.44774457]\n",
      "Iteration:  414  Training loss:  [0.46900576]  Validation loss:  [0.44949757]\n",
      "Iteration:  415  Training loss:  [0.46900976]  Validation loss:  [0.44872333]\n",
      "Iteration:  416  Training loss:  [0.46879734]  Validation loss:  [0.44997665]\n",
      "Iteration:  417  Training loss:  [0.46883636]  Validation loss:  [0.44933257]\n",
      "Iteration:  418  Training loss:  [0.46891024]  Validation loss:  [0.44738287]\n",
      "Iteration:  419  Training loss:  [0.46868627]  Validation loss:  [0.44840722]\n",
      "Iteration:  420  Training loss:  [0.46876078]  Validation loss:  [0.44675712]\n",
      "Iteration:  421  Training loss:  [0.46888116]  Validation loss:  [0.44518868]\n",
      "Iteration:  422  Training loss:  [0.46900731]  Validation loss:  [0.44391517]\n",
      "Iteration:  423  Training loss:  [0.46901537]  Validation loss:  [0.44335854]\n",
      "Iteration:  424  Training loss:  [0.46910167]  Validation loss:  [0.4430282]\n",
      "Iteration:  425  Training loss:  [0.46886206]  Validation loss:  [0.44358484]\n",
      "Iteration:  426  Training loss:  [0.46908742]  Validation loss:  [0.44317016]\n",
      "Iteration:  427  Training loss:  [0.46969081]  Validation loss:  [0.4425468]\n",
      "Iteration:  428  Training loss:  [0.47003443]  Validation loss:  [0.44078629]\n",
      "Iteration:  429  Training loss:  [0.46938759]  Validation loss:  [0.44154012]\n",
      "Iteration:  430  Training loss:  [0.47122932]  Validation loss:  [0.44113059]\n",
      "Iteration:  431  Training loss:  [0.46971384]  Validation loss:  [0.44235463]\n",
      "Iteration:  432  Training loss:  [0.46995077]  Validation loss:  [0.44038449]\n",
      "Iteration:  433  Training loss:  [0.47042468]  Validation loss:  [0.44027825]\n",
      "Iteration:  434  Training loss:  [0.47102897]  Validation loss:  [0.44039621]\n",
      "Iteration:  435  Training loss:  [0.47120129]  Validation loss:  [0.43893233]\n",
      "Iteration:  436  Training loss:  [0.46978327]  Validation loss:  [0.44145582]\n",
      "Iteration:  437  Training loss:  [0.46874471]  Validation loss:  [0.44421057]\n",
      "Iteration:  438  Training loss:  [0.46959741]  Validation loss:  [0.44396874]\n",
      "Iteration:  439  Training loss:  [0.46829199]  Validation loss:  [0.44534016]\n",
      "Iteration:  440  Training loss:  [0.46771412]  Validation loss:  [0.44761348]\n",
      "Iteration:  441  Training loss:  [0.46771046]  Validation loss:  [0.44586214]\n",
      "Iteration:  442  Training loss:  [0.46795104]  Validation loss:  [0.44566504]\n",
      "Iteration:  443  Training loss:  [0.46737541]  Validation loss:  [0.44875743]\n",
      "Iteration:  444  Training loss:  [0.46697367]  Validation loss:  [0.45096283]\n",
      "Iteration:  445  Training loss:  [0.46702635]  Validation loss:  [0.44777318]\n",
      "Iteration:  446  Training loss:  [0.46683369]  Validation loss:  [0.45025028]\n",
      "Iteration:  447  Training loss:  [0.46697781]  Validation loss:  [0.45284869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  448  Training loss:  [0.46746883]  Validation loss:  [0.45547374]\n",
      "Iteration:  449  Training loss:  [0.46756367]  Validation loss:  [0.45247895]\n",
      "Iteration:  450  Training loss:  [0.46778882]  Validation loss:  [0.45461857]\n",
      "Iteration:  451  Training loss:  [0.46771243]  Validation loss:  [0.45301101]\n",
      "Iteration:  452  Training loss:  [0.46767745]  Validation loss:  [0.45036501]\n",
      "Iteration:  453  Training loss:  [0.46796597]  Validation loss:  [0.45211981]\n",
      "Iteration:  454  Training loss:  [0.4672218]  Validation loss:  [0.4509485]\n",
      "Iteration:  455  Training loss:  [0.46747658]  Validation loss:  [0.45262297]\n",
      "Iteration:  456  Training loss:  [0.46797167]  Validation loss:  [0.45525111]\n",
      "Iteration:  457  Training loss:  [0.46856319]  Validation loss:  [0.45708644]\n",
      "Iteration:  458  Training loss:  [0.46847923]  Validation loss:  [0.45488076]\n",
      "Iteration:  459  Training loss:  [0.46829148]  Validation loss:  [0.45174754]\n",
      "Iteration:  460  Training loss:  [0.46835469]  Validation loss:  [0.44972979]\n",
      "Iteration:  461  Training loss:  [0.46724581]  Validation loss:  [0.44747339]\n",
      "Iteration:  462  Training loss:  [0.46639422]  Validation loss:  [0.44598543]\n",
      "Iteration:  463  Training loss:  [0.46644444]  Validation loss:  [0.44670203]\n",
      "Iteration:  464  Training loss:  [0.46672663]  Validation loss:  [0.4488297]\n",
      "Iteration:  465  Training loss:  [0.46671039]  Validation loss:  [0.44661597]\n",
      "Iteration:  466  Training loss:  [0.46612407]  Validation loss:  [0.44556685]\n",
      "Iteration:  467  Training loss:  [0.46646776]  Validation loss:  [0.44153373]\n",
      "Iteration:  468  Training loss:  [0.46641259]  Validation loss:  [0.44529017]\n",
      "Iteration:  469  Training loss:  [0.46652856]  Validation loss:  [0.4467295]\n",
      "Iteration:  470  Training loss:  [0.46624855]  Validation loss:  [0.44627222]\n",
      "Iteration:  471  Training loss:  [0.46564882]  Validation loss:  [0.44526598]\n",
      "Iteration:  472  Training loss:  [0.46571021]  Validation loss:  [0.44391911]\n",
      "Iteration:  473  Training loss:  [0.46570486]  Validation loss:  [0.44537338]\n",
      "Iteration:  474  Training loss:  [0.46589553]  Validation loss:  [0.44903271]\n",
      "Iteration:  475  Training loss:  [0.46511836]  Validation loss:  [0.44662792]\n",
      "Iteration:  476  Training loss:  [0.46526854]  Validation loss:  [0.44855376]\n",
      "Iteration:  477  Training loss:  [0.46556962]  Validation loss:  [0.45225851]\n",
      "Iteration:  478  Training loss:  [0.46496032]  Validation loss:  [0.45133605]\n",
      "Iteration:  479  Training loss:  [0.46468636]  Validation loss:  [0.45049108]\n",
      "Iteration:  480  Training loss:  [0.46486691]  Validation loss:  [0.45246169]\n",
      "Iteration:  481  Training loss:  [0.46453838]  Validation loss:  [0.45134832]\n",
      "Iteration:  482  Training loss:  [0.46464565]  Validation loss:  [0.45292821]\n",
      "Iteration:  483  Training loss:  [0.46451149]  Validation loss:  [0.45051102]\n",
      "Iteration:  484  Training loss:  [0.46482143]  Validation loss:  [0.45354169]\n",
      "Iteration:  485  Training loss:  [0.46556361]  Validation loss:  [0.45632506]\n",
      "Iteration:  486  Training loss:  [0.46489684]  Validation loss:  [0.45519705]\n",
      "Iteration:  487  Training loss:  [0.46485017]  Validation loss:  [0.4520011]\n",
      "Iteration:  488  Training loss:  [0.46423161]  Validation loss:  [0.45032249]\n",
      "Iteration:  489  Training loss:  [0.46440434]  Validation loss:  [0.45194174]\n",
      "Iteration:  490  Training loss:  [0.46464114]  Validation loss:  [0.45404352]\n",
      "Iteration:  491  Training loss:  [0.46408344]  Validation loss:  [0.45315023]\n",
      "Iteration:  492  Training loss:  [0.46394188]  Validation loss:  [0.45132678]\n",
      "Iteration:  493  Training loss:  [0.46366004]  Validation loss:  [0.45026047]\n",
      "Iteration:  494  Training loss:  [0.46355029]  Validation loss:  [0.44781664]\n",
      "Iteration:  495  Training loss:  [0.46349704]  Validation loss:  [0.44741222]\n",
      "Iteration:  496  Training loss:  [0.46355675]  Validation loss:  [0.44724039]\n",
      "Iteration:  497  Training loss:  [0.46365812]  Validation loss:  [0.4471484]\n",
      "Iteration:  498  Training loss:  [0.46328855]  Validation loss:  [0.44901875]\n",
      "Iteration:  499  Training loss:  [0.46338583]  Validation loss:  [0.44861925]\n",
      "Iteration:  500  Training loss:  [0.4637973]  Validation loss:  [0.44830528]\n",
      "Iteration:  501  Training loss:  [0.46345468]  Validation loss:  [0.44951655]\n",
      "Iteration:  502  Training loss:  [0.46322547]  Validation loss:  [0.45157819]\n",
      "Iteration:  503  Training loss:  [0.46355725]  Validation loss:  [0.45071558]\n",
      "Iteration:  504  Training loss:  [0.46355765]  Validation loss:  [0.44671922]\n",
      "Iteration:  505  Training loss:  [0.46326546]  Validation loss:  [0.44797219]\n",
      "Iteration:  506  Training loss:  [0.46298495]  Validation loss:  [0.44927732]\n",
      "Iteration:  507  Training loss:  [0.46309528]  Validation loss:  [0.44907759]\n",
      "Iteration:  508  Training loss:  [0.46360758]  Validation loss:  [0.44839558]\n",
      "Iteration:  509  Training loss:  [0.46303759]  Validation loss:  [0.4498077]\n",
      "Iteration:  510  Training loss:  [0.46291975]  Validation loss:  [0.4481109]\n",
      "Iteration:  511  Training loss:  [0.4628266]  Validation loss:  [0.44473321]\n",
      "Iteration:  512  Training loss:  [0.46245052]  Validation loss:  [0.44638817]\n",
      "Iteration:  513  Training loss:  [0.46279189]  Validation loss:  [0.44601696]\n",
      "Iteration:  514  Training loss:  [0.46230791]  Validation loss:  [0.44779609]\n",
      "Iteration:  515  Training loss:  [0.46225767]  Validation loss:  [0.45076734]\n",
      "Iteration:  516  Training loss:  [0.46236955]  Validation loss:  [0.45338262]\n",
      "Iteration:  517  Training loss:  [0.46286025]  Validation loss:  [0.45604955]\n",
      "Iteration:  518  Training loss:  [0.46385194]  Validation loss:  [0.45924107]\n",
      "Iteration:  519  Training loss:  [0.46340528]  Validation loss:  [0.45586163]\n",
      "Iteration:  520  Training loss:  [0.46382796]  Validation loss:  [0.45760314]\n",
      "Iteration:  521  Training loss:  [0.46487981]  Validation loss:  [0.46041516]\n",
      "Iteration:  522  Training loss:  [0.46368]  Validation loss:  [0.45823271]\n",
      "Iteration:  523  Training loss:  [0.46299424]  Validation loss:  [0.45707436]\n",
      "Iteration:  524  Training loss:  [0.46261013]  Validation loss:  [0.45387302]\n",
      "Iteration:  525  Training loss:  [0.46217331]  Validation loss:  [0.4530038]\n",
      "Iteration:  526  Training loss:  [0.46192658]  Validation loss:  [0.44940082]\n",
      "Iteration:  527  Training loss:  [0.46183677]  Validation loss:  [0.44748691]\n",
      "Iteration:  528  Training loss:  [0.46203493]  Validation loss:  [0.4490335]\n",
      "Iteration:  529  Training loss:  [0.46203629]  Validation loss:  [0.44680934]\n",
      "Iteration:  530  Training loss:  [0.46257996]  Validation loss:  [0.44909756]\n",
      "Iteration:  531  Training loss:  [0.46306641]  Validation loss:  [0.45196942]\n",
      "Iteration:  532  Training loss:  [0.4627598]  Validation loss:  [0.44866803]\n",
      "Iteration:  533  Training loss:  [0.46331457]  Validation loss:  [0.45123355]\n",
      "Iteration:  534  Training loss:  [0.46406607]  Validation loss:  [0.45319976]\n",
      "Iteration:  535  Training loss:  [0.46542806]  Validation loss:  [0.45620035]\n",
      "Iteration:  536  Training loss:  [0.46406088]  Validation loss:  [0.45407071]\n",
      "Iteration:  537  Training loss:  [0.46385598]  Validation loss:  [0.45163124]\n",
      "Iteration:  538  Training loss:  [0.46266185]  Validation loss:  [0.44965814]\n",
      "Iteration:  539  Training loss:  [0.46353248]  Validation loss:  [0.45216926]\n",
      "Iteration:  540  Training loss:  [0.46419473]  Validation loss:  [0.45404805]\n",
      "Iteration:  541  Training loss:  [0.4648205]  Validation loss:  [0.45554119]\n",
      "Iteration:  542  Training loss:  [0.46422553]  Validation loss:  [0.45120647]\n",
      "Iteration:  543  Training loss:  [0.46513289]  Validation loss:  [0.45326738]\n",
      "Iteration:  544  Training loss:  [0.46497843]  Validation loss:  [0.45062123]\n",
      "Iteration:  545  Training loss:  [0.46582583]  Validation loss:  [0.45337072]\n",
      "Iteration:  546  Training loss:  [0.46655007]  Validation loss:  [0.4552359]\n",
      "Iteration:  547  Training loss:  [0.46716778]  Validation loss:  [0.45732236]\n",
      "Iteration:  548  Training loss:  [0.46434324]  Validation loss:  [0.45299065]\n",
      "Iteration:  549  Training loss:  [0.46512422]  Validation loss:  [0.45608199]\n",
      "Iteration:  550  Training loss:  [0.4661147]  Validation loss:  [0.45892019]\n",
      "Iteration:  551  Training loss:  [0.46681293]  Validation loss:  [0.46063462]\n",
      "Iteration:  552  Training loss:  [0.46803804]  Validation loss:  [0.46369322]\n",
      "Iteration:  553  Training loss:  [0.46941641]  Validation loss:  [0.4665202]\n",
      "Iteration:  554  Training loss:  [0.47129612]  Validation loss:  [0.46978718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  555  Training loss:  [0.46930741]  Validation loss:  [0.46742622]\n",
      "Iteration:  556  Training loss:  [0.46764796]  Validation loss:  [0.46522564]\n",
      "Iteration:  557  Training loss:  [0.46912994]  Validation loss:  [0.46918798]\n",
      "Iteration:  558  Training loss:  [0.46801861]  Validation loss:  [0.46780273]\n",
      "Iteration:  559  Training loss:  [0.4691477]  Validation loss:  [0.47003751]\n",
      "Iteration:  560  Training loss:  [0.47122379]  Validation loss:  [0.47340073]\n",
      "Iteration:  561  Training loss:  [0.4733099]  Validation loss:  [0.47841007]\n",
      "Iteration:  562  Training loss:  [0.4700146]  Validation loss:  [0.47398998]\n",
      "Iteration:  563  Training loss:  [0.46839806]  Validation loss:  [0.47185799]\n",
      "Iteration:  564  Training loss:  [0.47082918]  Validation loss:  [0.47597683]\n",
      "Iteration:  565  Training loss:  [0.47239199]  Validation loss:  [0.47908307]\n",
      "Iteration:  566  Training loss:  [0.47393235]  Validation loss:  [0.48188698]\n",
      "Iteration:  567  Training loss:  [0.47589125]  Validation loss:  [0.48614651]\n",
      "Iteration:  568  Training loss:  [0.47510794]  Validation loss:  [0.48193607]\n",
      "Iteration:  569  Training loss:  [0.47275711]  Validation loss:  [0.47918997]\n",
      "Iteration:  570  Training loss:  [0.47160569]  Validation loss:  [0.47499733]\n",
      "Iteration:  571  Training loss:  [0.47277118]  Validation loss:  [0.47726265]\n",
      "Iteration:  572  Training loss:  [0.46951441]  Validation loss:  [0.47301147]\n",
      "Iteration:  573  Training loss:  [0.47169998]  Validation loss:  [0.47653599]\n",
      "Iteration:  574  Training loss:  [0.46820285]  Validation loss:  [0.4721164]\n",
      "Iteration:  575  Training loss:  [0.4674511]  Validation loss:  [0.46881738]\n",
      "Iteration:  576  Training loss:  [0.46963818]  Validation loss:  [0.47425447]\n",
      "Iteration:  577  Training loss:  [0.4689615]  Validation loss:  [0.47117517]\n",
      "Iteration:  578  Training loss:  [0.46716099]  Validation loss:  [0.46897532]\n",
      "Iteration:  579  Training loss:  [0.4693138]  Validation loss:  [0.47271237]\n",
      "Iteration:  580  Training loss:  [0.47018477]  Validation loss:  [0.47419915]\n",
      "Iteration:  581  Training loss:  [0.471969]  Validation loss:  [0.47833877]\n",
      "Iteration:  582  Training loss:  [0.47149176]  Validation loss:  [0.47440019]\n",
      "Iteration:  583  Training loss:  [0.47258912]  Validation loss:  [0.4765201]\n",
      "Iteration:  584  Training loss:  [0.47209094]  Validation loss:  [0.47281708]\n",
      "Iteration:  585  Training loss:  [0.47169564]  Validation loss:  [0.47028184]\n",
      "Iteration:  586  Training loss:  [0.47012849]  Validation loss:  [0.46459102]\n",
      "Iteration:  587  Training loss:  [0.47101362]  Validation loss:  [0.4660734]\n",
      "Iteration:  588  Training loss:  [0.47262138]  Validation loss:  [0.46939543]\n",
      "Iteration:  589  Training loss:  [0.47179626]  Validation loss:  [0.46546868]\n",
      "Iteration:  590  Training loss:  [0.46937359]  Validation loss:  [0.46275643]\n",
      "Iteration:  591  Training loss:  [0.46746343]  Validation loss:  [0.46033612]\n",
      "Iteration:  592  Training loss:  [0.46723006]  Validation loss:  [0.45829813]\n",
      "Iteration:  593  Training loss:  [0.46946476]  Validation loss:  [0.46232827]\n",
      "Iteration:  594  Training loss:  [0.47033668]  Validation loss:  [0.46379845]\n",
      "Iteration:  595  Training loss:  [0.46783639]  Validation loss:  [0.46066754]\n",
      "Iteration:  596  Training loss:  [0.4661307]  Validation loss:  [0.45849359]\n",
      "Iteration:  597  Training loss:  [0.46343839]  Validation loss:  [0.45480306]\n",
      "Iteration:  598  Training loss:  [0.46528965]  Validation loss:  [0.45829428]\n",
      "Iteration:  599  Training loss:  [0.46516871]  Validation loss:  [0.45641498]\n",
      "Iteration:  600  Training loss:  [0.46263509]  Validation loss:  [0.45331789]\n",
      "Iteration:  601  Training loss:  [0.46328789]  Validation loss:  [0.45501622]\n",
      "Iteration:  602  Training loss:  [0.4631938]  Validation loss:  [0.45317995]\n",
      "Iteration:  603  Training loss:  [0.46130692]  Validation loss:  [0.45106854]\n",
      "Iteration:  604  Training loss:  [0.46118718]  Validation loss:  [0.44935851]\n",
      "Iteration:  605  Training loss:  [0.46110394]  Validation loss:  [0.4477158]\n",
      "Iteration:  606  Training loss:  [0.46222438]  Validation loss:  [0.45063683]\n",
      "Iteration:  607  Training loss:  [0.46289167]  Validation loss:  [0.45290845]\n",
      "Iteration:  608  Training loss:  [0.46525441]  Validation loss:  [0.45685153]\n",
      "Iteration:  609  Training loss:  [0.46263083]  Validation loss:  [0.45405903]\n",
      "Iteration:  610  Training loss:  [0.46056061]  Validation loss:  [0.45101279]\n",
      "Iteration:  611  Training loss:  [0.45877704]  Validation loss:  [0.44816293]\n",
      "Iteration:  612  Training loss:  [0.45935553]  Validation loss:  [0.45045094]\n",
      "Iteration:  613  Training loss:  [0.45814596]  Validation loss:  [0.44845754]\n",
      "Iteration:  614  Training loss:  [0.45746758]  Validation loss:  [0.44723643]\n",
      "Iteration:  615  Training loss:  [0.45705154]  Validation loss:  [0.44387482]\n",
      "Iteration:  616  Training loss:  [0.45697545]  Validation loss:  [0.44146713]\n",
      "Iteration:  617  Training loss:  [0.45617657]  Validation loss:  [0.44005327]\n",
      "Iteration:  618  Training loss:  [0.45584781]  Validation loss:  [0.43931702]\n",
      "Iteration:  619  Training loss:  [0.45563181]  Validation loss:  [0.43832735]\n",
      "Iteration:  620  Training loss:  [0.45562422]  Validation loss:  [0.43984205]\n",
      "Iteration:  621  Training loss:  [0.45555877]  Validation loss:  [0.43823739]\n",
      "Iteration:  622  Training loss:  [0.45561163]  Validation loss:  [0.43918509]\n",
      "Iteration:  623  Training loss:  [0.45534844]  Validation loss:  [0.43853071]\n",
      "Iteration:  624  Training loss:  [0.455251]  Validation loss:  [0.43826321]\n",
      "Iteration:  625  Training loss:  [0.45534005]  Validation loss:  [0.4403582]\n",
      "Iteration:  626  Training loss:  [0.45542937]  Validation loss:  [0.44208571]\n",
      "Iteration:  627  Training loss:  [0.45514666]  Validation loss:  [0.44140205]\n",
      "Iteration:  628  Training loss:  [0.45504651]  Validation loss:  [0.44078454]\n",
      "Iteration:  629  Training loss:  [0.45513902]  Validation loss:  [0.44275756]\n",
      "Iteration:  630  Training loss:  [0.45527497]  Validation loss:  [0.44426185]\n",
      "Iteration:  631  Training loss:  [0.45552989]  Validation loss:  [0.44583802]\n",
      "Iteration:  632  Training loss:  [0.45531014]  Validation loss:  [0.44341515]\n",
      "Iteration:  633  Training loss:  [0.45520102]  Validation loss:  [0.44157104]\n",
      "Iteration:  634  Training loss:  [0.45483125]  Validation loss:  [0.44064285]\n",
      "Iteration:  635  Training loss:  [0.45524691]  Validation loss:  [0.44285104]\n",
      "Iteration:  636  Training loss:  [0.45478984]  Validation loss:  [0.44162286]\n",
      "Iteration:  637  Training loss:  [0.45451368]  Validation loss:  [0.44084006]\n",
      "Iteration:  638  Training loss:  [0.45468938]  Validation loss:  [0.44280695]\n",
      "Iteration:  639  Training loss:  [0.4543916]  Validation loss:  [0.4417636]\n",
      "Iteration:  640  Training loss:  [0.45460836]  Validation loss:  [0.44375727]\n",
      "Iteration:  641  Training loss:  [0.45434035]  Validation loss:  [0.44312666]\n",
      "Iteration:  642  Training loss:  [0.45424016]  Validation loss:  [0.44259224]\n",
      "Iteration:  643  Training loss:  [0.45427528]  Validation loss:  [0.44209516]\n",
      "Iteration:  644  Training loss:  [0.45471246]  Validation loss:  [0.44143741]\n",
      "Iteration:  645  Training loss:  [0.45448996]  Validation loss:  [0.44259346]\n",
      "Iteration:  646  Training loss:  [0.45497048]  Validation loss:  [0.44185049]\n",
      "Iteration:  647  Training loss:  [0.4546756]  Validation loss:  [0.44293406]\n",
      "Iteration:  648  Training loss:  [0.45444826]  Validation loss:  [0.44419482]\n",
      "Iteration:  649  Training loss:  [0.45425086]  Validation loss:  [0.44631593]\n",
      "Iteration:  650  Training loss:  [0.45448262]  Validation loss:  [0.44938148]\n",
      "Iteration:  651  Training loss:  [0.45396867]  Validation loss:  [0.44576868]\n",
      "Iteration:  652  Training loss:  [0.45420537]  Validation loss:  [0.44774963]\n",
      "Iteration:  653  Training loss:  [0.45376778]  Validation loss:  [0.4439182]\n",
      "Iteration:  654  Training loss:  [0.45406707]  Validation loss:  [0.44587169]\n",
      "Iteration:  655  Training loss:  [0.45491228]  Validation loss:  [0.44883172]\n",
      "Iteration:  656  Training loss:  [0.455466]  Validation loss:  [0.45047838]\n",
      "Iteration:  657  Training loss:  [0.45455903]  Validation loss:  [0.44880944]\n",
      "Iteration:  658  Training loss:  [0.45401479]  Validation loss:  [0.44749743]\n",
      "Iteration:  659  Training loss:  [0.45495898]  Validation loss:  [0.45267103]\n",
      "Iteration:  660  Training loss:  [0.45560388]  Validation loss:  [0.45501894]\n",
      "Iteration:  661  Training loss:  [0.45617509]  Validation loss:  [0.45675701]\n",
      "Iteration:  662  Training loss:  [0.45533955]  Validation loss:  [0.45181108]\n",
      "Iteration:  663  Training loss:  [0.45421912]  Validation loss:  [0.449735]\n",
      "Iteration:  664  Training loss:  [0.45373832]  Validation loss:  [0.44644722]\n",
      "Iteration:  665  Training loss:  [0.45408417]  Validation loss:  [0.44803264]\n",
      "Iteration:  666  Training loss:  [0.45451446]  Validation loss:  [0.44967266]\n",
      "Iteration:  667  Training loss:  [0.45423348]  Validation loss:  [0.44768365]\n",
      "Iteration:  668  Training loss:  [0.45334977]  Validation loss:  [0.44577697]\n",
      "Iteration:  669  Training loss:  [0.45365314]  Validation loss:  [0.44743573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  670  Training loss:  [0.45336027]  Validation loss:  [0.44696487]\n",
      "Iteration:  671  Training loss:  [0.45304025]  Validation loss:  [0.44463453]\n",
      "Iteration:  672  Training loss:  [0.45269856]  Validation loss:  [0.44381741]\n",
      "Iteration:  673  Training loss:  [0.45287929]  Validation loss:  [0.44534953]\n",
      "Iteration:  674  Training loss:  [0.45251675]  Validation loss:  [0.44225147]\n",
      "Iteration:  675  Training loss:  [0.45270293]  Validation loss:  [0.44342088]\n",
      "Iteration:  676  Training loss:  [0.45236239]  Validation loss:  [0.44241218]\n",
      "Iteration:  677  Training loss:  [0.45268402]  Validation loss:  [0.44443544]\n",
      "Iteration:  678  Training loss:  [0.45245966]  Validation loss:  [0.44246801]\n",
      "Iteration:  679  Training loss:  [0.45308129]  Validation loss:  [0.44490651]\n",
      "Iteration:  680  Training loss:  [0.45257606]  Validation loss:  [0.44400568]\n",
      "Iteration:  681  Training loss:  [0.45207869]  Validation loss:  [0.44233051]\n",
      "Iteration:  682  Training loss:  [0.45191451]  Validation loss:  [0.44179584]\n",
      "Iteration:  683  Training loss:  [0.45219473]  Validation loss:  [0.44403775]\n",
      "Iteration:  684  Training loss:  [0.45197367]  Validation loss:  [0.44340235]\n",
      "Iteration:  685  Training loss:  [0.45197744]  Validation loss:  [0.44197087]\n",
      "Iteration:  686  Training loss:  [0.4527596]  Validation loss:  [0.44102204]\n",
      "Iteration:  687  Training loss:  [0.45358047]  Validation loss:  [0.44062502]\n",
      "Iteration:  688  Training loss:  [0.45268348]  Validation loss:  [0.44207381]\n",
      "Iteration:  689  Training loss:  [0.45328177]  Validation loss:  [0.44184446]\n",
      "Iteration:  690  Training loss:  [0.45426593]  Validation loss:  [0.44234803]\n",
      "Iteration:  691  Training loss:  [0.45464359]  Validation loss:  [0.4424209]\n",
      "Iteration:  692  Training loss:  [0.4534357]  Validation loss:  [0.44373092]\n",
      "Iteration:  693  Training loss:  [0.45302119]  Validation loss:  [0.4453363]\n",
      "Iteration:  694  Training loss:  [0.45256887]  Validation loss:  [0.44672957]\n",
      "Iteration:  695  Training loss:  [0.45304785]  Validation loss:  [0.44603043]\n",
      "Iteration:  696  Training loss:  [0.45263565]  Validation loss:  [0.44715418]\n",
      "Iteration:  697  Training loss:  [0.45283438]  Validation loss:  [0.45070715]\n",
      "Iteration:  698  Training loss:  [0.45296206]  Validation loss:  [0.45259381]\n",
      "Iteration:  699  Training loss:  [0.45230963]  Validation loss:  [0.44921725]\n",
      "Iteration:  700  Training loss:  [0.45238395]  Validation loss:  [0.44873071]\n",
      "Iteration:  701  Training loss:  [0.45305369]  Validation loss:  [0.44762781]\n",
      "Iteration:  702  Training loss:  [0.45353066]  Validation loss:  [0.44772668]\n",
      "Iteration:  703  Training loss:  [0.4532205]  Validation loss:  [0.44872916]\n",
      "Iteration:  704  Training loss:  [0.45305762]  Validation loss:  [0.45008634]\n",
      "Iteration:  705  Training loss:  [0.45288789]  Validation loss:  [0.45167734]\n",
      "Iteration:  706  Training loss:  [0.4527472]  Validation loss:  [0.45342324]\n",
      "Iteration:  707  Training loss:  [0.45218868]  Validation loss:  [0.45088562]\n",
      "Iteration:  708  Training loss:  [0.45225565]  Validation loss:  [0.45232462]\n",
      "Iteration:  709  Training loss:  [0.45245379]  Validation loss:  [0.45413125]\n",
      "Iteration:  710  Training loss:  [0.45235699]  Validation loss:  [0.45357176]\n",
      "Iteration:  711  Training loss:  [0.45240356]  Validation loss:  [0.4527833]\n",
      "Iteration:  712  Training loss:  [0.45184974]  Validation loss:  [0.4502587]\n",
      "Iteration:  713  Training loss:  [0.45189659]  Validation loss:  [0.45167477]\n",
      "Iteration:  714  Training loss:  [0.45226844]  Validation loss:  [0.45438675]\n",
      "Iteration:  715  Training loss:  [0.45219746]  Validation loss:  [0.45375144]\n",
      "Iteration:  716  Training loss:  [0.45230407]  Validation loss:  [0.45267643]\n",
      "Iteration:  717  Training loss:  [0.45231667]  Validation loss:  [0.45495779]\n",
      "Iteration:  718  Training loss:  [0.45245243]  Validation loss:  [0.45608453]\n",
      "Iteration:  719  Training loss:  [0.45272855]  Validation loss:  [0.45758625]\n",
      "Iteration:  720  Training loss:  [0.45329887]  Validation loss:  [0.4598329]\n",
      "Iteration:  721  Training loss:  [0.45284395]  Validation loss:  [0.45861329]\n",
      "Iteration:  722  Training loss:  [0.45379335]  Validation loss:  [0.46269343]\n",
      "Iteration:  723  Training loss:  [0.45305393]  Validation loss:  [0.4597195]\n",
      "Iteration:  724  Training loss:  [0.452861]  Validation loss:  [0.45926518]\n",
      "Iteration:  725  Training loss:  [0.45269647]  Validation loss:  [0.45855372]\n",
      "Iteration:  726  Training loss:  [0.45183979]  Validation loss:  [0.4548286]\n",
      "Iteration:  727  Training loss:  [0.45203036]  Validation loss:  [0.45616928]\n",
      "Iteration:  728  Training loss:  [0.45191635]  Validation loss:  [0.45495536]\n",
      "Iteration:  729  Training loss:  [0.45224801]  Validation loss:  [0.4534915]\n",
      "Iteration:  730  Training loss:  [0.45220395]  Validation loss:  [0.45532356]\n",
      "Iteration:  731  Training loss:  [0.45231878]  Validation loss:  [0.454929]\n",
      "Iteration:  732  Training loss:  [0.45221053]  Validation loss:  [0.45574477]\n",
      "Iteration:  733  Training loss:  [0.45136108]  Validation loss:  [0.45227575]\n",
      "Iteration:  734  Training loss:  [0.45154848]  Validation loss:  [0.45416643]\n",
      "Iteration:  735  Training loss:  [0.45162146]  Validation loss:  [0.45394413]\n",
      "Iteration:  736  Training loss:  [0.4508295]  Validation loss:  [0.44972722]\n",
      "Iteration:  737  Training loss:  [0.45095445]  Validation loss:  [0.45156472]\n",
      "Iteration:  738  Training loss:  [0.45049599]  Validation loss:  [0.44937225]\n",
      "Iteration:  739  Training loss:  [0.44969816]  Validation loss:  [0.44445247]\n",
      "Iteration:  740  Training loss:  [0.44966503]  Validation loss:  [0.44583215]\n",
      "Iteration:  741  Training loss:  [0.44923916]  Validation loss:  [0.44345206]\n",
      "Iteration:  742  Training loss:  [0.44933781]  Validation loss:  [0.44285503]\n",
      "Iteration:  743  Training loss:  [0.44937694]  Validation loss:  [0.44500692]\n",
      "Iteration:  744  Training loss:  [0.44945169]  Validation loss:  [0.44463065]\n",
      "Iteration:  745  Training loss:  [0.44942043]  Validation loss:  [0.44600945]\n",
      "Iteration:  746  Training loss:  [0.44948035]  Validation loss:  [0.44548448]\n",
      "Iteration:  747  Training loss:  [0.44978787]  Validation loss:  [0.44492038]\n",
      "Iteration:  748  Training loss:  [0.44926457]  Validation loss:  [0.4416683]\n",
      "Iteration:  749  Training loss:  [0.44892466]  Validation loss:  [0.43991053]\n",
      "Iteration:  750  Training loss:  [0.44864117]  Validation loss:  [0.44185259]\n",
      "Iteration:  751  Training loss:  [0.44862484]  Validation loss:  [0.44316718]\n",
      "Iteration:  752  Training loss:  [0.4487383]  Validation loss:  [0.44220338]\n",
      "Iteration:  753  Training loss:  [0.44864079]  Validation loss:  [0.44339895]\n",
      "Iteration:  754  Training loss:  [0.44824162]  Validation loss:  [0.44054997]\n",
      "Iteration:  755  Training loss:  [0.44834382]  Validation loss:  [0.44283683]\n",
      "Iteration:  756  Training loss:  [0.44775777]  Validation loss:  [0.43826918]\n",
      "Iteration:  757  Training loss:  [0.44825804]  Validation loss:  [0.43648533]\n",
      "Iteration:  758  Training loss:  [0.44801316]  Validation loss:  [0.43771214]\n",
      "Iteration:  759  Training loss:  [0.44786034]  Validation loss:  [0.43946758]\n",
      "Iteration:  760  Training loss:  [0.44792953]  Validation loss:  [0.4390026]\n",
      "Iteration:  761  Training loss:  [0.44791032]  Validation loss:  [0.44101443]\n",
      "Iteration:  762  Training loss:  [0.44802338]  Validation loss:  [0.44231948]\n",
      "Iteration:  763  Training loss:  [0.44884966]  Validation loss:  [0.44735803]\n",
      "Iteration:  764  Training loss:  [0.44808298]  Validation loss:  [0.44298231]\n",
      "Iteration:  765  Training loss:  [0.44794373]  Validation loss:  [0.4418107]\n",
      "Iteration:  766  Training loss:  [0.44813962]  Validation loss:  [0.4440794]\n",
      "Iteration:  767  Training loss:  [0.44799947]  Validation loss:  [0.44345255]\n",
      "Iteration:  768  Training loss:  [0.44807834]  Validation loss:  [0.44289319]\n",
      "Iteration:  769  Training loss:  [0.44772074]  Validation loss:  [0.44084103]\n",
      "Iteration:  770  Training loss:  [0.44773109]  Validation loss:  [0.44225982]\n",
      "Iteration:  771  Training loss:  [0.4471223]  Validation loss:  [0.43844466]\n",
      "Iteration:  772  Training loss:  [0.44688478]  Validation loss:  [0.43668379]\n",
      "Iteration:  773  Training loss:  [0.44659103]  Validation loss:  [0.43398732]\n",
      "Iteration:  774  Training loss:  [0.44638147]  Validation loss:  [0.42917417]\n",
      "Iteration:  775  Training loss:  [0.44642259]  Validation loss:  [0.42875016]\n",
      "Iteration:  776  Training loss:  [0.44675811]  Validation loss:  [0.42791188]\n",
      "Iteration:  777  Training loss:  [0.44712223]  Validation loss:  [0.42751961]\n",
      "Iteration:  778  Training loss:  [0.44792501]  Validation loss:  [0.4272456]\n",
      "Iteration:  779  Training loss:  [0.44887194]  Validation loss:  [0.42699698]\n",
      "Iteration:  780  Training loss:  [0.4488459]  Validation loss:  [0.42561796]\n",
      "Iteration:  781  Training loss:  [0.44803038]  Validation loss:  [0.42747633]\n",
      "Iteration:  782  Training loss:  [0.44872103]  Validation loss:  [0.42763041]\n",
      "Iteration:  783  Training loss:  [0.44964112]  Validation loss:  [0.42801546]\n",
      "Iteration:  784  Training loss:  [0.44858731]  Validation loss:  [0.42948574]\n",
      "Iteration:  785  Training loss:  [0.44859312]  Validation loss:  [0.42798765]\n",
      "Iteration:  786  Training loss:  [0.44789817]  Validation loss:  [0.42893663]\n",
      "Iteration:  787  Training loss:  [0.44708255]  Validation loss:  [0.42980102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  788  Training loss:  [0.44627603]  Validation loss:  [0.43153976]\n",
      "Iteration:  789  Training loss:  [0.44627143]  Validation loss:  [0.43598182]\n",
      "Iteration:  790  Training loss:  [0.44643536]  Validation loss:  [0.43951594]\n",
      "Iteration:  791  Training loss:  [0.44648317]  Validation loss:  [0.4408877]\n",
      "Iteration:  792  Training loss:  [0.44660855]  Validation loss:  [0.44237568]\n",
      "Iteration:  793  Training loss:  [0.44655142]  Validation loss:  [0.44147041]\n",
      "Iteration:  794  Training loss:  [0.44660896]  Validation loss:  [0.44279792]\n",
      "Iteration:  795  Training loss:  [0.4458912]  Validation loss:  [0.43776253]\n",
      "Iteration:  796  Training loss:  [0.44595408]  Validation loss:  [0.43920751]\n",
      "Iteration:  797  Training loss:  [0.44591492]  Validation loss:  [0.43888329]\n",
      "Iteration:  798  Training loss:  [0.44598285]  Validation loss:  [0.44022557]\n",
      "Iteration:  799  Training loss:  [0.44628829]  Validation loss:  [0.44247252]\n",
      "Iteration:  800  Training loss:  [0.44610345]  Validation loss:  [0.44171769]\n",
      "Iteration:  801  Training loss:  [0.44605508]  Validation loss:  [0.44089781]\n",
      "Iteration:  802  Training loss:  [0.44560089]  Validation loss:  [0.4381436]\n",
      "Iteration:  803  Training loss:  [0.44578308]  Validation loss:  [0.43731159]\n",
      "Iteration:  804  Training loss:  [0.44544222]  Validation loss:  [0.43500388]\n",
      "Iteration:  805  Training loss:  [0.44526928]  Validation loss:  [0.43690232]\n",
      "Iteration:  806  Training loss:  [0.44489103]  Validation loss:  [0.43372082]\n",
      "Iteration:  807  Training loss:  [0.44495865]  Validation loss:  [0.43560958]\n",
      "Iteration:  808  Training loss:  [0.44537623]  Validation loss:  [0.43789195]\n",
      "Iteration:  809  Training loss:  [0.44493783]  Validation loss:  [0.43467064]\n",
      "Iteration:  810  Training loss:  [0.44476546]  Validation loss:  [0.4342262]\n",
      "Iteration:  811  Training loss:  [0.44454527]  Validation loss:  [0.43325518]\n",
      "Iteration:  812  Training loss:  [0.44469022]  Validation loss:  [0.43510993]\n",
      "Iteration:  813  Training loss:  [0.44455234]  Validation loss:  [0.43384063]\n",
      "Iteration:  814  Training loss:  [0.44457682]  Validation loss:  [0.43515523]\n",
      "Iteration:  815  Training loss:  [0.4445225]  Validation loss:  [0.43463929]\n",
      "Iteration:  816  Training loss:  [0.44465175]  Validation loss:  [0.43668444]\n",
      "Iteration:  817  Training loss:  [0.44471113]  Validation loss:  [0.43729589]\n",
      "Iteration:  818  Training loss:  [0.44454283]  Validation loss:  [0.43646089]\n",
      "Iteration:  819  Training loss:  [0.44493418]  Validation loss:  [0.43891106]\n",
      "Iteration:  820  Training loss:  [0.44586484]  Validation loss:  [0.44194869]\n",
      "Iteration:  821  Training loss:  [0.44606622]  Validation loss:  [0.44264251]\n",
      "Iteration:  822  Training loss:  [0.44653811]  Validation loss:  [0.44426519]\n",
      "Iteration:  823  Training loss:  [0.44695611]  Validation loss:  [0.44538442]\n",
      "Iteration:  824  Training loss:  [0.44622796]  Validation loss:  [0.44436168]\n",
      "Iteration:  825  Training loss:  [0.44555422]  Validation loss:  [0.44333116]\n",
      "Iteration:  826  Training loss:  [0.44484766]  Validation loss:  [0.4404128]\n",
      "Iteration:  827  Training loss:  [0.44421743]  Validation loss:  [0.43657648]\n",
      "Iteration:  828  Training loss:  [0.44418364]  Validation loss:  [0.43576263]\n",
      "Iteration:  829  Training loss:  [0.44377946]  Validation loss:  [0.43232336]\n",
      "Iteration:  830  Training loss:  [0.44376164]  Validation loss:  [0.43363416]\n",
      "Iteration:  831  Training loss:  [0.44377648]  Validation loss:  [0.43318152]\n",
      "Iteration:  832  Training loss:  [0.4439004]  Validation loss:  [0.43288968]\n",
      "Iteration:  833  Training loss:  [0.44438604]  Validation loss:  [0.43258344]\n",
      "Iteration:  834  Training loss:  [0.44380788]  Validation loss:  [0.428972]\n",
      "Iteration:  835  Training loss:  [0.44352867]  Validation loss:  [0.42979188]\n",
      "Iteration:  836  Training loss:  [0.4439035]  Validation loss:  [0.42933733]\n",
      "Iteration:  837  Training loss:  [0.44363662]  Validation loss:  [0.43048623]\n",
      "Iteration:  838  Training loss:  [0.44343436]  Validation loss:  [0.43168819]\n",
      "Iteration:  839  Training loss:  [0.44323383]  Validation loss:  [0.42922494]\n",
      "Iteration:  840  Training loss:  [0.44323588]  Validation loss:  [0.43287303]\n",
      "Iteration:  841  Training loss:  [0.44335316]  Validation loss:  [0.43252506]\n",
      "Iteration:  842  Training loss:  [0.44325718]  Validation loss:  [0.43412583]\n",
      "Iteration:  843  Training loss:  [0.44329529]  Validation loss:  [0.43582117]\n",
      "Iteration:  844  Training loss:  [0.44363209]  Validation loss:  [0.43388025]\n",
      "Iteration:  845  Training loss:  [0.44342987]  Validation loss:  [0.43208213]\n",
      "Iteration:  846  Training loss:  [0.44314399]  Validation loss:  [0.4339679]\n",
      "Iteration:  847  Training loss:  [0.44315792]  Validation loss:  [0.43526478]\n",
      "Iteration:  848  Training loss:  [0.44314245]  Validation loss:  [0.43507755]\n",
      "Iteration:  849  Training loss:  [0.443363]  Validation loss:  [0.43738248]\n",
      "Iteration:  850  Training loss:  [0.44327296]  Validation loss:  [0.43712132]\n",
      "Iteration:  851  Training loss:  [0.44289884]  Validation loss:  [0.4346451]\n",
      "Iteration:  852  Training loss:  [0.44305795]  Validation loss:  [0.43594093]\n",
      "Iteration:  853  Training loss:  [0.44346647]  Validation loss:  [0.43828433]\n",
      "Iteration:  854  Training loss:  [0.44450967]  Validation loss:  [0.44138536]\n",
      "Iteration:  855  Training loss:  [0.44400767]  Validation loss:  [0.44042495]\n",
      "Iteration:  856  Training loss:  [0.44442627]  Validation loss:  [0.44199864]\n",
      "Iteration:  857  Training loss:  [0.44395258]  Validation loss:  [0.43934815]\n",
      "Iteration:  858  Training loss:  [0.44324771]  Validation loss:  [0.43561882]\n",
      "Iteration:  859  Training loss:  [0.44251082]  Validation loss:  [0.43020165]\n",
      "Iteration:  860  Training loss:  [0.44219528]  Validation loss:  [0.42620315]\n",
      "Iteration:  861  Training loss:  [0.44214296]  Validation loss:  [0.42438475]\n",
      "Iteration:  862  Training loss:  [0.44288874]  Validation loss:  [0.42714419]\n",
      "Iteration:  863  Training loss:  [0.44339607]  Validation loss:  [0.42949768]\n",
      "Iteration:  864  Training loss:  [0.44308774]  Validation loss:  [0.4290385]\n",
      "Iteration:  865  Training loss:  [0.44223129]  Validation loss:  [0.4272691]\n",
      "Iteration:  866  Training loss:  [0.44187701]  Validation loss:  [0.42655322]\n",
      "Iteration:  867  Training loss:  [0.44219366]  Validation loss:  [0.42802933]\n",
      "Iteration:  868  Training loss:  [0.44213837]  Validation loss:  [0.42679282]\n",
      "Iteration:  869  Training loss:  [0.4419399]  Validation loss:  [0.42450594]\n",
      "Iteration:  870  Training loss:  [0.44187614]  Validation loss:  [0.42316558]\n",
      "Iteration:  871  Training loss:  [0.4412577]  Validation loss:  [0.4222963]\n",
      "Iteration:  872  Training loss:  [0.44129922]  Validation loss:  [0.41939242]\n",
      "Iteration:  873  Training loss:  [0.44137352]  Validation loss:  [0.42069278]\n",
      "Iteration:  874  Training loss:  [0.44183724]  Validation loss:  [0.42291362]\n",
      "Iteration:  875  Training loss:  [0.44127694]  Validation loss:  [0.42202401]\n",
      "Iteration:  876  Training loss:  [0.44146459]  Validation loss:  [0.42342105]\n",
      "Iteration:  877  Training loss:  [0.4409395]  Validation loss:  [0.4219782]\n",
      "Iteration:  878  Training loss:  [0.44088201]  Validation loss:  [0.42054169]\n",
      "Iteration:  879  Training loss:  [0.4408986]  Validation loss:  [0.41945709]\n",
      "Iteration:  880  Training loss:  [0.44107614]  Validation loss:  [0.42129642]\n",
      "Iteration:  881  Training loss:  [0.44112101]  Validation loss:  [0.41927316]\n",
      "Iteration:  882  Training loss:  [0.44163689]  Validation loss:  [0.42176403]\n",
      "Iteration:  883  Training loss:  [0.44083909]  Validation loss:  [0.42035181]\n",
      "Iteration:  884  Training loss:  [0.44036952]  Validation loss:  [0.41962996]\n",
      "Iteration:  885  Training loss:  [0.44037404]  Validation loss:  [0.41801714]\n",
      "Iteration:  886  Training loss:  [0.44038528]  Validation loss:  [0.41934233]\n",
      "Iteration:  887  Training loss:  [0.44054485]  Validation loss:  [0.42108536]\n",
      "Iteration:  888  Training loss:  [0.44020167]  Validation loss:  [0.42039613]\n",
      "Iteration:  889  Training loss:  [0.44006052]  Validation loss:  [0.4200013]\n",
      "Iteration:  890  Training loss:  [0.4402368]  Validation loss:  [0.41805462]\n",
      "Iteration:  891  Training loss:  [0.43996365]  Validation loss:  [0.41958226]\n",
      "Iteration:  892  Training loss:  [0.43995297]  Validation loss:  [0.41820112]\n",
      "Iteration:  893  Training loss:  [0.43985584]  Validation loss:  [0.41974462]\n",
      "Iteration:  894  Training loss:  [0.43978547]  Validation loss:  [0.41923752]\n",
      "Iteration:  895  Training loss:  [0.43973853]  Validation loss:  [0.42120881]\n",
      "Iteration:  896  Training loss:  [0.43964482]  Validation loss:  [0.42080623]\n",
      "Iteration:  897  Training loss:  [0.43960888]  Validation loss:  [0.42316555]\n",
      "Iteration:  898  Training loss:  [0.43951356]  Validation loss:  [0.42175616]\n",
      "Iteration:  899  Training loss:  [0.43943866]  Validation loss:  [0.42011571]\n",
      "Iteration:  900  Training loss:  [0.43939126]  Validation loss:  [0.41962713]\n",
      "Iteration:  901  Training loss:  [0.43936484]  Validation loss:  [0.41822728]\n",
      "Iteration:  902  Training loss:  [0.43943499]  Validation loss:  [0.41577557]\n",
      "Iteration:  903  Training loss:  [0.43957164]  Validation loss:  [0.41354118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  904  Training loss:  [0.43969655]  Validation loss:  [0.41182373]\n",
      "Iteration:  905  Training loss:  [0.43946053]  Validation loss:  [0.4129783]\n",
      "Iteration:  906  Training loss:  [0.43930448]  Validation loss:  [0.41463035]\n",
      "Iteration:  907  Training loss:  [0.43966803]  Validation loss:  [0.41161584]\n",
      "Iteration:  908  Training loss:  [0.43940929]  Validation loss:  [0.41575037]\n",
      "Iteration:  909  Training loss:  [0.43926635]  Validation loss:  [0.41547996]\n",
      "Iteration:  910  Training loss:  [0.43934786]  Validation loss:  [0.41348581]\n",
      "Iteration:  911  Training loss:  [0.43974488]  Validation loss:  [0.41278803]\n",
      "Iteration:  912  Training loss:  [0.44013398]  Validation loss:  [0.41257174]\n",
      "Iteration:  913  Training loss:  [0.44021949]  Validation loss:  [0.41112165]\n",
      "Iteration:  914  Training loss:  [0.44178652]  Validation loss:  [0.41103031]\n",
      "Iteration:  915  Training loss:  [0.44238604]  Validation loss:  [0.41109545]\n",
      "Iteration:  916  Training loss:  [0.44161378]  Validation loss:  [0.41179427]\n",
      "Iteration:  917  Training loss:  [0.44007024]  Validation loss:  [0.41280981]\n",
      "Iteration:  918  Training loss:  [0.44012459]  Validation loss:  [0.41169583]\n",
      "Iteration:  919  Training loss:  [0.44092355]  Validation loss:  [0.41139051]\n",
      "Iteration:  920  Training loss:  [0.4400013]  Validation loss:  [0.41245952]\n",
      "Iteration:  921  Training loss:  [0.43934529]  Validation loss:  [0.41438478]\n",
      "Iteration:  922  Training loss:  [0.43893851]  Validation loss:  [0.41508795]\n",
      "Iteration:  923  Training loss:  [0.43907075]  Validation loss:  [0.41236061]\n",
      "Iteration:  924  Training loss:  [0.43917262]  Validation loss:  [0.41071866]\n",
      "Iteration:  925  Training loss:  [0.4398296]  Validation loss:  [0.41025568]\n",
      "Iteration:  926  Training loss:  [0.43914514]  Validation loss:  [0.41114246]\n",
      "Iteration:  927  Training loss:  [0.44011968]  Validation loss:  [0.4111432]\n",
      "Iteration:  928  Training loss:  [0.44086494]  Validation loss:  [0.41108291]\n",
      "Iteration:  929  Training loss:  [0.44083366]  Validation loss:  [0.40921684]\n",
      "Iteration:  930  Training loss:  [0.43975757]  Validation loss:  [0.41044794]\n",
      "Iteration:  931  Training loss:  [0.43882046]  Validation loss:  [0.41168033]\n",
      "Iteration:  932  Training loss:  [0.43840825]  Validation loss:  [0.41264257]\n",
      "Iteration:  933  Training loss:  [0.43881936]  Validation loss:  [0.41232147]\n",
      "Iteration:  934  Training loss:  [0.43842099]  Validation loss:  [0.41324363]\n",
      "Iteration:  935  Training loss:  [0.43804876]  Validation loss:  [0.41429261]\n",
      "Iteration:  936  Training loss:  [0.43834488]  Validation loss:  [0.41102538]\n",
      "Iteration:  937  Training loss:  [0.43868021]  Validation loss:  [0.41064618]\n",
      "Iteration:  938  Training loss:  [0.43983713]  Validation loss:  [0.41009311]\n",
      "Iteration:  939  Training loss:  [0.4408041]  Validation loss:  [0.41002509]\n",
      "Iteration:  940  Training loss:  [0.43974284]  Validation loss:  [0.4105973]\n",
      "Iteration:  941  Training loss:  [0.44231794]  Validation loss:  [0.41081571]\n",
      "Iteration:  942  Training loss:  [0.44142885]  Validation loss:  [0.4116237]\n",
      "Iteration:  943  Training loss:  [0.43953252]  Validation loss:  [0.41237971]\n",
      "Iteration:  944  Training loss:  [0.44051952]  Validation loss:  [0.41220997]\n",
      "Iteration:  945  Training loss:  [0.44054579]  Validation loss:  [0.4102438]\n",
      "Iteration:  946  Training loss:  [0.44094624]  Validation loss:  [0.40788357]\n",
      "Iteration:  947  Training loss:  [0.43938672]  Validation loss:  [0.40879533]\n",
      "Iteration:  948  Training loss:  [0.4385199]  Validation loss:  [0.41022469]\n",
      "Iteration:  949  Training loss:  [0.43792419]  Validation loss:  [0.41099929]\n",
      "Iteration:  950  Training loss:  [0.43799168]  Validation loss:  [0.40991629]\n",
      "Iteration:  951  Training loss:  [0.43730855]  Validation loss:  [0.4116641]\n",
      "Iteration:  952  Training loss:  [0.43706503]  Validation loss:  [0.41326792]\n",
      "Iteration:  953  Training loss:  [0.436956]  Validation loss:  [0.41479505]\n",
      "Iteration:  954  Training loss:  [0.43704966]  Validation loss:  [0.4122893]\n",
      "Iteration:  955  Training loss:  [0.43707335]  Validation loss:  [0.41132592]\n",
      "Iteration:  956  Training loss:  [0.43709117]  Validation loss:  [0.41119953]\n",
      "Iteration:  957  Training loss:  [0.43714299]  Validation loss:  [0.41010811]\n",
      "Iteration:  958  Training loss:  [0.43721615]  Validation loss:  [0.40880619]\n",
      "Iteration:  959  Training loss:  [0.43748412]  Validation loss:  [0.40690359]\n",
      "Iteration:  960  Training loss:  [0.43776927]  Validation loss:  [0.40642107]\n",
      "Iteration:  961  Training loss:  [0.4371684]  Validation loss:  [0.40786744]\n",
      "Iteration:  962  Training loss:  [0.43735577]  Validation loss:  [0.40640824]\n",
      "Iteration:  963  Training loss:  [0.4377383]  Validation loss:  [0.405704]\n",
      "Iteration:  964  Training loss:  [0.43789844]  Validation loss:  [0.40456044]\n",
      "Iteration:  965  Training loss:  [0.43801432]  Validation loss:  [0.40453758]\n",
      "Iteration:  966  Training loss:  [0.43866413]  Validation loss:  [0.40418579]\n",
      "Iteration:  967  Training loss:  [0.43903877]  Validation loss:  [0.40412699]\n",
      "Iteration:  968  Training loss:  [0.43979989]  Validation loss:  [0.40409644]\n",
      "Iteration:  969  Training loss:  [0.44039318]  Validation loss:  [0.40210434]\n",
      "Iteration:  970  Training loss:  [0.43900983]  Validation loss:  [0.40284297]\n",
      "Iteration:  971  Training loss:  [0.43861608]  Validation loss:  [0.40313845]\n",
      "Iteration:  972  Training loss:  [0.43917487]  Validation loss:  [0.40300642]\n",
      "Iteration:  973  Training loss:  [0.43944517]  Validation loss:  [0.40301923]\n",
      "Iteration:  974  Training loss:  [0.44013189]  Validation loss:  [0.40302077]\n",
      "Iteration:  975  Training loss:  [0.43870168]  Validation loss:  [0.40377528]\n",
      "Iteration:  976  Training loss:  [0.43969263]  Validation loss:  [0.40403237]\n",
      "Iteration:  977  Training loss:  [0.4385721]  Validation loss:  [0.40563411]\n",
      "Iteration:  978  Training loss:  [0.4386497]  Validation loss:  [0.40333239]\n",
      "Iteration:  979  Training loss:  [0.4390113]  Validation loss:  [0.40334381]\n",
      "Iteration:  980  Training loss:  [0.4375345]  Validation loss:  [0.40448623]\n",
      "Iteration:  981  Training loss:  [0.43789635]  Validation loss:  [0.40271527]\n",
      "Iteration:  982  Training loss:  [0.43710751]  Validation loss:  [0.4035069]\n",
      "Iteration:  983  Training loss:  [0.43744474]  Validation loss:  [0.40344309]\n",
      "Iteration:  984  Training loss:  [0.43645546]  Validation loss:  [0.40461925]\n",
      "Iteration:  985  Training loss:  [0.43591934]  Validation loss:  [0.40599235]\n",
      "Iteration:  986  Training loss:  [0.43653961]  Validation loss:  [0.40569433]\n",
      "Iteration:  987  Training loss:  [0.43710676]  Validation loss:  [0.4054747]\n",
      "Iteration:  988  Training loss:  [0.43604814]  Validation loss:  [0.40821159]\n",
      "Iteration:  989  Training loss:  [0.43647419]  Validation loss:  [0.40823823]\n",
      "Iteration:  990  Training loss:  [0.43683719]  Validation loss:  [0.40810881]\n",
      "Iteration:  991  Training loss:  [0.43834146]  Validation loss:  [0.40792513]\n",
      "Iteration:  992  Training loss:  [0.4373403]  Validation loss:  [0.40897328]\n",
      "Iteration:  993  Training loss:  [0.43757095]  Validation loss:  [0.40734898]\n",
      "Iteration:  994  Training loss:  [0.43758919]  Validation loss:  [0.40598782]\n",
      "Iteration:  995  Training loss:  [0.4362307]  Validation loss:  [0.40697431]\n",
      "Iteration:  996  Training loss:  [0.43647011]  Validation loss:  [0.40696057]\n",
      "Iteration:  997  Training loss:  [0.43571593]  Validation loss:  [0.40874248]\n",
      "Iteration:  998  Training loss:  [0.43696524]  Validation loss:  [0.40831709]\n",
      "Iteration:  999  Training loss:  [0.43606195]  Validation loss:  [0.40897864]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "# you can run this block multiple times to improve results, and play around with decreasing the learning rate later on\n",
    "for i in range(1000):\n",
    "    # Calculate Gradient using backwards operations and update parameters\n",
    "    # Don't forget to zero the gradient afterwards! \n",
    "    \n",
    "    network.zero_grad()\n",
    "    \n",
    "    element = np.random.randint(len(y_train), size=1)\n",
    "    element = element[0]\n",
    "    \n",
    "    data = Tensor(np.expand_dims(x_train[:,element], axis=1))\n",
    "    target = y_train[element]\n",
    "    \n",
    "    prediction = network.forward(data)\n",
    "    l = loss.forward(prediction, target)\n",
    "    \n",
    "    l.backward()\n",
    "    \n",
    "    network.linear.W = Tensor(network.linear.W.data - learning_rate * network.linear.W.grad, requires_grad=True)\n",
    "    network.linear.b = Tensor(network.linear.b.data - learning_rate * network.linear.b.grad, requires_grad=True)\n",
    "    \n",
    "    # Print results\n",
    "    training_loss = loss_on_dataset(x_train, y_train)\n",
    "    validation_loss = loss_on_dataset(x_val, y_val)\n",
    "\n",
    "    print(\"Iteration: \", i, \" Training loss: \", training_loss, \" Validation loss: \", validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on the Test Set\n",
    "\n",
    "After you are happy with your validation loss, you can test your model on the test data. In reality you are not supposed to retrain or further refine your model after looking at the test data. In practice, many people do it anyway. However, this is extremely bad practice, and means your test data is validation data only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  [0.39556643]\n"
     ]
    }
   ],
   "source": [
    "test_loss = loss_on_dataset(x_test, y_test)\n",
    "print(\"Test loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is rather easy, there will probably not be much difference in training/validation/test performance. And because we shuffle the data in the beginning, the exact results on each set will vary a little bit if you restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
