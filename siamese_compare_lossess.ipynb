{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g29n-03XdMc"
   },
   "source": [
    "# Image similarity estimation using a Siamese Network with a contrastive loss\n",
    "\n",
    "**Author:** Mehdi<br>\n",
    "**Date created:** 2021/05/06<br>\n",
    "**Last modified:** 2022/09/10<br>\n",
    "**Description:** Similarity learning using a siamese network trained with a contrastive loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQQrxvyNXdMf"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKpF_xBfXdMg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from emnist import extract_training_samples\n",
    "from emnist import extract_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 42\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcZA2jgDXdMh"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzoUvjowXdMh"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "DIMENSION = 20\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "margin = 1  # Margin for contrastive loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DehTegg-XdMh"
   },
   "source": [
    "## Load the (E)MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LN9uH0AXdMi"
   },
   "outputs": [],
   "source": [
    "### https://pypi.org/project/emnist/ ###\n",
    "### https://stackoverflow.com/questions/51125969/loading-emnist-letters-dataset ###\n",
    "\n",
    "images_train_val, labels_train_val = extract_training_samples('balanced')\n",
    "images_test, labels_test = extract_test_samples('balanced')\n",
    "\n",
    "print(\"Train Val Dataset shape:\",images_train_val.shape, labels_train_val.shape)\n",
    "idx = np.argsort(labels_train_val)\n",
    "images_train_val_sorted = images_train_val[idx]\n",
    "labels_train_val_sorted = labels_train_val[idx]\n",
    "print(\"Sorted train val labels:\",labels_train_val_sorted)\n",
    "\n",
    "print(\"Test Dataset shape:\",images_test.shape, labels_test.shape)\n",
    "idx = np.argsort(labels_test)\n",
    "images_test_sorted = images_test[idx]\n",
    "labels_test_sorted = labels_test[idx]\n",
    "print(\"Sorted test labels:\",labels_test_sorted)\n",
    "\n",
    "divide_position = 36 # labels 36 to 46 => 10 unseen\n",
    "index_divide_position = labels_train_val_sorted.tolist().index(divide_position)\n",
    "\n",
    "print(\"Position start of label\", divide_position, \":\", index_divide_position)\n",
    "\n",
    "# Drop labels from train_val set\n",
    "x_train_val = images_train_val_sorted[:index_divide_position]\n",
    "y_train_val = labels_train_val_sorted[:index_divide_position]\n",
    "\n",
    "print(\"Train Val Dataset shape with droped unseen labels:\",x_train_val.shape, y_train_val.shape)\n",
    "print(\"Traub vak labels after drop:\",y_train_val)\n",
    "\n",
    "#shuffle train_val x and y but with the same order\n",
    "\n",
    "xy = list(zip(x_train_val, y_train_val))\n",
    "\n",
    "random.shuffle(xy)\n",
    "\n",
    "x_train_val, y_train_val = zip(*xy)\n",
    "\n",
    "# Define test dataset\n",
    "\n",
    "x_test = images_test_sorted\n",
    "y_test = labels_test_sorted\n",
    "\n",
    "# Change the data type to a floating point format\n",
    "x_train_val = np.array(x_train_val).astype(\"float32\")\n",
    "x_test = np.array(x_test).astype(\"float32\")\n",
    "\n",
    "y_train_val = np.array(y_train_val)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvSgU34IXdMi"
   },
   "source": [
    "## Define training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhkfPg1yXdMj"
   },
   "outputs": [],
   "source": [
    "print(\"Train+Val len:\",len(x_train_val),len(y_train_val))\n",
    "print(\"Test len:\",len(x_test),len(y_test))\n",
    "\n",
    "# 50 training data, 30% validation data, 20% test data \n",
    "\n",
    "x_train, x_val = x_train_val[:47000], x_train_val[47000:75200]\n",
    "y_train, y_val = y_train_val[:47000], y_train_val[47000:75200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train len:\",len(x_train),len(y_train))\n",
    "print(\"Val len:\",len(x_val),len(y_val))\n",
    "print(\"Test len:\",len(x_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m92thOe8XdMj"
   },
   "source": [
    "## Create pairs of images\n",
    "\n",
    "We will train the model to differentiate between digits of different classes. For\n",
    "example, digit `0` needs to be differentiated from the rest of the\n",
    "digits (`1` through `9`), digit `1` - from `0` and `2` through `9`, and so on.\n",
    "To carry this out, we will select N random images from class A (for example,\n",
    "for digit `0`) and pair them with N random images from another class B\n",
    "(for example, for digit `1`). Then, we can repeat this process for all classes\n",
    "of digits (until digit `9`). Once we have paired digit `0` with other digits,\n",
    "we can repeat this process for the remaining classes for the rest of the digits\n",
    "(from `1` until `9`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InH67YntXdMj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_pairs(x, y):\n",
    "    \"\"\"Creates a tuple containing image pairs with corresponding label.\n",
    "\n",
    "    Arguments:\n",
    "        x: List containing images, each index in this list corresponds to one image.\n",
    "        y: List containing labels, each label with datatype of `int`.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing two numpy arrays as (pairs_of_samples, labels),\n",
    "        where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and\n",
    "        labels are a binary array of shape (2len(x)).\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = max(y) + 1\n",
    "    min_value = min(y)\n",
    "    digit_indices = [np.where(y == i)[0] for i in range(min_value, num_classes)]\n",
    "    \n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for idx1 in range(len(x)):\n",
    "        # add a matching example\n",
    "        x1 = x[idx1]\n",
    "        label1 = y[idx1]\n",
    "        idx2 = random.choice(digit_indices[label1-min_value])\n",
    "        x2 = x[idx2]\n",
    "\n",
    "        pairs += [[x1, x2]]\n",
    "        labels += [0]\n",
    "\n",
    "        # add a non-matching example\n",
    "        label2 = random.randint(0,len(np.unique((y)))-1)\n",
    "        while label2 == label1: \n",
    "            label2 = random.randint(0,len(np.unique((y)))-1)\n",
    "\n",
    "\n",
    "        idx2 = random.choice(digit_indices[label2])\n",
    "   \n",
    "        x2 = x[idx2]\n",
    "\n",
    "        pairs += [[x1, x2]]\n",
    "        labels += [1]\n",
    "\n",
    "    return np.array(pairs), np.array(labels).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train = make_pairs(x_train, y_train)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val = make_pairs(x_val, y_val)\n",
    "\n",
    "# make test pairs\n",
    "pairs_test, labels_test = make_pairs(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YLwd9xmXdMj"
   },
   "source": [
    "We get:\n",
    "\n",
    "**pairs_train.shape = (150400, 2, 28, 28)**\n",
    "\n",
    "- We have 150400 pairs\n",
    "- Each pair contains 2 images\n",
    "- Each image has shape `(28, 28)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1JtEUBuXdMk"
   },
   "source": [
    "Split the training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYcwnpZJXdMk"
   },
   "outputs": [],
   "source": [
    "x_train_1 = pairs_train[:, 0]  # x_train_1.shape is (60000, 28, 28)\n",
    "x_train_2 = pairs_train[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMOk7mH-XdMk"
   },
   "source": [
    "Split the validation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_L3Nj3bXdMk"
   },
   "outputs": [],
   "source": [
    "x_val_1 = pairs_val[:, 0]  # x_val_1.shape = (60000, 28, 28)\n",
    "x_val_2 = pairs_val[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJmT0sdJXdMk"
   },
   "source": [
    "Split the test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmNBJHDNXdMk"
   },
   "outputs": [],
   "source": [
    "x_test_1 = pairs_test[:, 0]  # x_test_1.shape = (20000, 28, 28)\n",
    "x_test_2 = pairs_test[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pORk-ebIXdMk"
   },
   "source": [
    "## Visualize pairs and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEo_u-ppXdMl"
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualize(pairs, labels, to_show=6, num_col=3, predictions=None, test=False):\n",
    "    \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.\n",
    "\n",
    "    Arguments:\n",
    "        pairs: Numpy Array, of pairs to visualize, having shape\n",
    "               (Number of pairs, 2, 28, 28).\n",
    "        to_show: Int, number of examples to visualize (default is 6)\n",
    "                `to_show` must be an integral multiple of `num_col`.\n",
    "                 Otherwise it will be trimmed if it is greater than num_col,\n",
    "                 and incremented if if it is less then num_col.\n",
    "        num_col: Int, number of images in one row - (default is 3)\n",
    "                 For test and train respectively, it should not exceed 3 and 7.\n",
    "        predictions: Numpy Array of predictions with shape (to_show, 1) -\n",
    "                     (default is None)\n",
    "                     Must be passed when test=True.\n",
    "        test: Boolean telling whether the dataset being visualized is\n",
    "              train dataset or test dataset - (default False).\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define num_row\n",
    "    # If to_show % num_col != 0\n",
    "    #    trim to_show,\n",
    "    #       to trim to_show limit num_row to the point where\n",
    "    #       to_show % num_col == 0\n",
    "    #\n",
    "    # If to_show//num_col == 0\n",
    "    #    then it means num_col is greater then to_show\n",
    "    #    increment to_show\n",
    "    #       to increment to_show set num_row to 1\n",
    "    num_row = to_show // num_col if to_show // num_col != 0 else 1\n",
    "\n",
    "    # `to_show` must be an integral multiple of `num_col`\n",
    "    #  we found num_row and we have num_col\n",
    "    #  to increment or decrement to_show\n",
    "    #  to make it integral multiple of `num_col`\n",
    "    #  simply set it equal to num_row * num_col\n",
    "    to_show = num_row * num_col\n",
    "\n",
    "    # Plot the images\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(5, 5))\n",
    "    for i in range(to_show):\n",
    "\n",
    "        # If the number of rows is 1, the axes array is one-dimensional\n",
    "        if num_row == 1:\n",
    "            ax = axes[i % num_col]\n",
    "        else:\n",
    "            ax = axes[i // num_col, i % num_col]\n",
    "\n",
    "        ax.imshow(tf.concat([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\")\n",
    "        ax.set_axis_off()\n",
    "        if test:\n",
    "            ax.set_title(\"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0]))\n",
    "        else:\n",
    "            ax.set_title(\"Label: {}\".format(labels[i]))\n",
    "    if test:\n",
    "        plt.tight_layout(rect=(0, 0, 1.9, 1.9), w_pad=0.0)\n",
    "    else:\n",
    "        plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_zqOMPDXdMl"
   },
   "source": [
    "Inspect training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WU6acLb7XdMl"
   },
   "outputs": [],
   "source": [
    "visualize(pairs_train[:-1], labels_train[:-1], to_show=4, num_col=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFnrhWNZXdMl"
   },
   "source": [
    "Inspect validation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSqBkoLBXdMm"
   },
   "outputs": [],
   "source": [
    "visualize(pairs_val[:-1], labels_val[:-1], to_show=4, num_col=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuhP5LPmXdMm"
   },
   "source": [
    "Inspect test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQ7zFI0YXdMm"
   },
   "outputs": [],
   "source": [
    "visualize(pairs_test[:-1], labels_test[:-1], to_show=4, num_col=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSaJkzwhXdMm"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "There are two input layers, each leading to its own network, which\n",
    "produces embeddings. A `Lambda` layer then merges them using an\n",
    "[Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) and the\n",
    "merged output is fed to the final network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                              monitor='val_loss',\n",
    "                              mode='min',\n",
    "                              verbose = 1,\n",
    "                              restore_best_weights=True)\n",
    "callback=[early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1ZBwJHJXdMm"
   },
   "outputs": [],
   "source": [
    "# CONTRASTIVE LOSS\n",
    "\n",
    "# Provided two tensors t1 and t2\n",
    "# Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "input = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x = tf.keras.layers.BatchNormalization()(input)\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
    "x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(DIMENSION, activation=\"relu\")(x)\n",
    "embedding_network = keras.Model(input, x)\n",
    "\n",
    "\n",
    "input_1 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "input_2 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1 = embedding_network(input_1)\n",
    "tower_2 = embedding_network(input_2)\n",
    "\n",
    "merge_layer = layers.Lambda(euclidean_distance)([tower_1, tower_2])\n",
    "normal_layer = tf.keras.layers.BatchNormalization()(merge_layer)\n",
    "output_layer = layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n",
    "\n",
    "# define model for each loss function\n",
    "siamese1 = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICE LOSS\n",
    "\n",
    "# Provided two tensors t1 and t2\n",
    "# Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "input2 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x2 = tf.keras.layers.BatchNormalization()(input2)\n",
    "x2 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x2)\n",
    "x2 = layers.AveragePooling2D(pool_size=(2, 2))(x2)\n",
    "x2 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x2)\n",
    "x2 = layers.AveragePooling2D(pool_size=(2, 2))(x2)\n",
    "x2 = layers.Flatten()(x2)\n",
    "\n",
    "x2 = tf.keras.layers.BatchNormalization()(x2)\n",
    "x2 = layers.Dense(47, activation=\"relu\")(x2)\n",
    "embedding_network_2 = keras.Model(input2, x2)\n",
    "\n",
    "\n",
    "input_1_2 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "input_2_2 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1_2 = embedding_network_2(input_1_2)\n",
    "tower_2_2 = embedding_network_2(input_2_2)\n",
    "\n",
    "merge_layer_2 = layers.Lambda(euclidean_distance)([tower_1_2, tower_2_2])\n",
    "normal_layer_2 = tf.keras.layers.BatchNormalization()(merge_layer_2)\n",
    "output_layer_2 = layers.Dense(1, activation=\"sigmoid\")(normal_layer_2)\n",
    "siamese2 = keras.Model(inputs=[input_1_2, input_2_2], outputs=output_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE LOSS\n",
    "\n",
    "# Provided two tensors t1 and t2\n",
    "# Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "input3 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x3 = tf.keras.layers.BatchNormalization()(input3)\n",
    "x3 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x3)\n",
    "x3 = layers.AveragePooling2D(pool_size=(2, 2))(x3)\n",
    "x3 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x3)\n",
    "x3 = layers.AveragePooling2D(pool_size=(2, 2))(x3)\n",
    "x3 = layers.Flatten()(x3)\n",
    "\n",
    "x3 = tf.keras.layers.BatchNormalization()(x3)\n",
    "x3 = layers.Dense(47, activation=\"relu\")(x3)\n",
    "embedding_network_3 = keras.Model(input3, x3)\n",
    "\n",
    "\n",
    "input_1_3 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "input_2_3 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1_3 = embedding_network_3(input_1_3)\n",
    "tower_2_3 = embedding_network_3(input_2_3)\n",
    "\n",
    "merge_layer_3 = layers.Lambda(euclidean_distance)([tower_1_3, tower_2_3])\n",
    "normal_layer_3 = tf.keras.layers.BatchNormalization()(merge_layer_3)\n",
    "output_layer_3 = layers.Dense(1, activation=\"sigmoid\")(normal_layer_3)\n",
    "siamese3 = keras.Model(inputs=[input_1_3, input_2_3], outputs=output_layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE + DICE LOSS\n",
    "\n",
    "# Provided two tensors t1 and t2\n",
    "# Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "input4 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x4 = tf.keras.layers.BatchNormalization()(input4)\n",
    "x4 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x4)\n",
    "x4 = layers.AveragePooling2D(pool_size=(2, 2))(x4)\n",
    "x4 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x4)\n",
    "x4 = layers.AveragePooling2D(pool_size=(2, 2))(x4)\n",
    "x4 = layers.Flatten()(x4)\n",
    "\n",
    "x4 = tf.keras.layers.BatchNormalization()(x4)\n",
    "x4 = layers.Dense(47, activation=\"relu\")(x4)\n",
    "embedding_network_4 = keras.Model(input4, x4)\n",
    "\n",
    "\n",
    "input_1_4 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "input_2_4 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1_4 = embedding_network_4(input_1_4)\n",
    "tower_2_4 = embedding_network_4(input_2_4)\n",
    "\n",
    "merge_layer_4 = layers.Lambda(euclidean_distance)([tower_1_4, tower_2_4])\n",
    "normal_layer_4 = tf.keras.layers.BatchNormalization()(merge_layer_4)\n",
    "output_layer_4 = layers.Dense(1, activation=\"sigmoid\")(normal_layer_4)\n",
    "siamese4 = keras.Model(inputs=[input_1_4, input_2_4], outputs=output_layer_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HINGE LOSS\n",
    "\n",
    "# Provided two tensors t1 and t2\n",
    "# Euclidean distance = sqrt(sum(square(t1-t2)))\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True)\n",
    "    return tf.math.sqrt(tf.math.maximum(sum_square, tf.keras.backend.epsilon()))\n",
    "\n",
    "\n",
    "input5 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "x5 = tf.keras.layers.BatchNormalization()(input5)\n",
    "x5 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x5)\n",
    "x5 = layers.AveragePooling2D(pool_size=(2, 2))(x5)\n",
    "x5 = layers.Conv2D(32, (3, 3), activation=\"relu\")(x5)\n",
    "x5 = layers.AveragePooling2D(pool_size=(2, 2))(x5)\n",
    "x5 = layers.Flatten()(x5)\n",
    "\n",
    "x5 = tf.keras.layers.BatchNormalization()(x5)\n",
    "x5 = layers.Dense(47, activation=\"relu\")(x5)\n",
    "embedding_network_5 = keras.Model(input5, x5)\n",
    "\n",
    "\n",
    "input_1_5 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "input_2_5 = layers.Input((IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "\n",
    "# As mentioned above, Siamese Network share weights between\n",
    "# tower networks (sister networks). To allow this, we will use\n",
    "# same embedding network for both tower networks.\n",
    "tower_1_5 = embedding_network_5(input_1_5)\n",
    "tower_2_5 = embedding_network_5(input_2_5)\n",
    "\n",
    "merge_layer_5 = layers.Lambda(euclidean_distance)([tower_1_5, tower_2_5])\n",
    "normal_layer_5 = tf.keras.layers.BatchNormalization()(merge_layer_5)\n",
    "output_layer_5 = layers.Dense(1, activation=\"sigmoid\")(normal_layer_5)\n",
    "siamese5 = keras.Model(inputs=[input_1_5, input_2_5], outputs=output_layer_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAUk9kMwXdMm"
   },
   "source": [
    "## Define the contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9VNvPX6XdMm"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(margin=1):\n",
    "    \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        'contrastive_loss' function with data ('margin') attached.\n",
    "    \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the contrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing contrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )   \n",
    "\n",
    "        \"\"\"\n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * margin_square + (y_true) * square_pred\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "    return contrastive_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Dice Loss\n",
    "Code: https://stackoverflow.com/questions/72195156/correct-implementation-of-dice-loss-in-tensorflow-keras\n",
    "\n",
    "![Dice Loss](./img/Dice_Loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Loss\n",
    "def dice_loss(margin=1): #ignore margin\n",
    "\n",
    "    smooth=1e-6\n",
    "\n",
    "    def dice_coef(y_true, y_pred, smooth=100):        \n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        return dice\n",
    "    \n",
    "    def dice_coef_loss(y_true, y_pred):\n",
    "        return 1 - dice_coef(y_true, y_pred)\n",
    "    \n",
    "    return dice_coef_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Binary Cross Entropy Loss\n",
    "Code: https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "\n",
    "![BCE Loss](./img/BCE_Loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE_Loss\n",
    "def BCE_loss(margin=1): #ignore margin\n",
    "\n",
    "    def binary_cross_entropy_loss(y_true, y_pred):\n",
    "        BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "        return BCE\n",
    "    \n",
    "    return binary_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the BCE + Dice Loss\n",
    "Paper: https://ieeexplore.ieee.org/abstract/document/10023508?casa_token=_zhnKjSaYgYAAAAA:zZ0oLIx6Yoc56OeYEdvdJfwA0FLyQyhg3NNmd8YOUTzPLLgn5hK8ZoJ4UUAwH_9Dn55YLj2fQbk \n",
    "Code: https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch/notebook\n",
    "\n",
    "![Dice Loss](./img/BCE_Loss.png)\n",
    "![Dice Loss](./img/Dice_Loss.png)\n",
    "![Dice Loss](./img/BCE_Dice_Loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE_Dice_Loss\n",
    "def BCE_dice_loss(margin=1): #ignore margin\n",
    "\n",
    "    smooth=1e-6\n",
    "\n",
    "    def dice_coef_loss(y_true, y_pred):\n",
    "        y_true_f = K.flatten(y_true)\n",
    "        y_pred_f = K.flatten(y_pred)\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "        \n",
    "        return dice\n",
    "\n",
    "    \n",
    "    def binary_cross_entropy_loss(y_true, y_pred):\n",
    "        BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "        return BCE\n",
    "    \n",
    "\n",
    "    def BCE_dice_coef_loss(y_true, y_pred):\n",
    "        dice_loss = 1 - dice_coef_loss(y_true, y_pred)\n",
    "        BCE =  binary_cross_entropy_loss(y_true, y_pred)\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        return Dice_BCE\n",
    "    \n",
    "    return BCE_dice_coef_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guc3DfIYXdMn"
   },
   "source": [
    "## Compile the model with the contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJ6As8asXdMn"
   },
   "outputs": [],
   "source": [
    "siamese1.compile(loss=contrastive_loss(margin=margin), optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "siamese1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model with the dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "siamese2.compile(loss=dice_loss(margin=margin), optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "siamese2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model with the BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese3.compile(loss=BCE_loss(margin=margin), optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "siamese3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model with the BCE + dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese4.compile(loss=BCE_dice_loss(margin=margin), optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "siamese4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model with the Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese5.compile(loss=tf.keras.losses.Hinge(), optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "siamese5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARGpf7KEXdMn"
   },
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvErqxixXdMn"
   },
   "outputs": [],
   "source": [
    "# Contrastive Loss\n",
    "history1 = siamese1.fit(\n",
    "    [x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    callbacks=callback,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice Loss\n",
    "history2 = siamese2.fit(\n",
    "    [x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    callbacks=callback,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE Loss\n",
    "history3 = siamese3.fit(\n",
    "    [x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    callbacks=callback,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE + Dice Loss\n",
    "history4 = siamese4.fit(\n",
    "    [x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    callbacks=callback,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge Loss\n",
    "history5 = siamese5.fit(\n",
    "    [x_train_1, x_train_2],\n",
    "    labels_train,\n",
    "    validation_data=([x_val_1, x_val_2], labels_val),\n",
    "    callbacks=callback,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5iZCH93XdMn"
   },
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metric(history, metric, title, has_valid=True, x = 0, y = 0):\n",
    "    \"\"\"Plots the given 'metric' from 'history'.\n",
    "\n",
    "    Arguments:\n",
    "        history: history attribute of History object returned from Model.fit.\n",
    "        metric: Metric to plot, a string value present as key in 'history'.\n",
    "        title: A string to be used as title of plot.\n",
    "        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    axs[x, y].plot(history[metric])\n",
    "    if has_valid:\n",
    "        axs[x, y].plot(history[\"val_\" + metric])\n",
    "        axs[x, y].legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    axs[x, y].set_title(title)\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel=\"epoch\")\n",
    "        \n",
    "    axs.flat[0].set(ylabel=\"accuracy\")\n",
    "    axs.flat[6].set(ylabel=\"accuracy\")\n",
    "    axs.flat[3].set(ylabel=\"loss\")\n",
    "    axs.flat[9].set(ylabel=\"loss\")\n",
    "    \n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    #for ax in axs.flat:\n",
    "    #    ax.label_outer()\n",
    "        \n",
    "    axs[x,y].set_xlim([0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3)\n",
    "\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# Plot the accuracy of contrastive loss\n",
    "plt_metric(history=history1.history, metric=\"accuracy\", title=\"Model accuracy of CL\", x=0 ,y=0)\n",
    "\n",
    "# Plot the contrastive loss\n",
    "plt_metric(history=history1.history, metric=\"loss\", title=\"Contrastive Loss\", x=1 ,y=0)\n",
    "\n",
    "# Plot the accuracy of dice loss\n",
    "plt_metric(history=history2.history, metric=\"accuracy\", title=\"Model accuracy of DL\",x=0 ,y=1)\n",
    "\n",
    "# Plot the dice loss\n",
    "plt_metric(history=history2.history, metric=\"loss\", title=\"Dice Loss\", x=1 ,y=1)\n",
    "\n",
    "# Plot the accuracy of BCE loss\n",
    "plt_metric(history=history3.history, metric=\"accuracy\", title=\"Model accuracy of BCEL\",x=0 ,y=2)\n",
    "\n",
    "# Plot the BCE loss\n",
    "plt_metric(history=history3.history, metric=\"loss\", title=\"BCE Loss\", x=1 ,y=2)\n",
    "\n",
    "# Plot the accuracy of BCE dice loss\n",
    "plt_metric(history=history4.history, metric=\"accuracy\", title=\"Model accuracy of BCEDL\", x=2 ,y=0)\n",
    "\n",
    "# Plot the BCE dice loss\n",
    "plt_metric(history=history4.history, metric=\"loss\", title=\"BCE Dice Loss\", x=3 ,y=0)\n",
    "\n",
    "# Plot the accuracy of hinge loss\n",
    "plt_metric(history=history5.history, metric=\"accuracy\", title=\"Model accuracy of Hinge\", x=2 ,y=2)\n",
    "\n",
    "# Plot the hinge loss\n",
    "plt_metric(history=history5.history, metric=\"loss\", title=\"Hinge Loss\", x=3 ,y=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_metric_train(history, metric, title, has_valid=True, x = 0, y = 0):\n",
    "    \"\"\"Plots the given 'metric' from 'history'.\n",
    "\n",
    "    Arguments:\n",
    "        history: history attribute of History object returned from Model.fit.\n",
    "        metric: Metric to plot, a string value present as key in 'history'.\n",
    "        title: A string to be used as title of plot.\n",
    "        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    axs[x,y].plot(history[metric])\n",
    "    \n",
    "    axs[x,y].legend([\"CL\",\"DL\",\"BCEL\",\"BCEDL\",\"Hinge\"], loc=\"upper left\")\n",
    "    \n",
    "    axs[x,y].set_title(title)\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel=\"epoch\")\n",
    "    \n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    #for ax in axs.flat:\n",
    "    #    ax.label_outer()  \n",
    "    \n",
    "    axs[x,y].set_xlim([0, 100])\n",
    "            \n",
    "def plt_metric_val(history, metric, title, has_valid=True, x = 0, y = 0):\n",
    "    \"\"\"Plots the given 'metric' from 'history'.\n",
    "\n",
    "    Arguments:\n",
    "        history: history attribute of History object returned from Model.fit.\n",
    "        metric: Metric to plot, a string value present as key in 'history'.\n",
    "        title: A string to be used as title of plot.\n",
    "        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    axs[x,y].plot(history[\"val_\" + metric])\n",
    "    \n",
    "    axs[x,y].legend([\"CL\",\"DL\",\"BCEL\",\"BCEDL\",\"Hinge\"], loc=\"upper left\")\n",
    "    \n",
    "    axs[x,y].set_title(title)\n",
    "        \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel=\"epoch\")\n",
    "                        \n",
    "    # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "    #for ax in axs.flat:\n",
    "    #    ax.label_outer()\n",
    "    \n",
    "    axs[x,y].set_xlim([0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2)\n",
    "\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "# Plot the accuracy of contrastive loss\n",
    "plt_metric_train(history=history1.history, metric=\"accuracy\", title=\"Train Accuracy\", x=0 ,y=0)\n",
    "\n",
    "# Plot the accuracy of dice loss\n",
    "plt_metric_train(history=history2.history, metric=\"accuracy\", title=\"Train Accuracy\",x=0 ,y=0)\n",
    "\n",
    "# Plot the accuracy of BCE loss\n",
    "plt_metric_train(history=history3.history, metric=\"accuracy\", title=\"Train Accuracy\",x=0 ,y=0)\n",
    "\n",
    "# Plot the accuracy of BCE dice loss\n",
    "plt_metric_train(history=history4.history, metric=\"accuracy\", title=\"Train Accuracy\", x=0 ,y=0)\n",
    "\n",
    "# Plot the accuracy of hinge loss\n",
    "plt_metric_train(history=history6.history, metric=\"accuracy\", title=\"Train Accuracy\", x=0 ,y=0)\n",
    "\n",
    "\n",
    "# Plot the contrastive loss\n",
    "plt_metric_train(history=history1.history, metric=\"loss\", title=\"Train Loss\", x=1 ,y=0)\n",
    "\n",
    "# Plot the dice loss\n",
    "plt_metric_train(history=history2.history, metric=\"loss\", title=\"Train Loss\", x=1 ,y=0)\n",
    "\n",
    "# Plot the BCE loss\n",
    "plt_metric_train(history=history3.history, metric=\"loss\", title=\"Train Loss\", x=1 ,y=0)\n",
    "\n",
    "# Plot the BCE dice loss\n",
    "plt_metric_train(history=history4.history, metric=\"loss\", title=\"Train Loss\", x=1 ,y=0)\n",
    "\n",
    "# Plot the hinge loss\n",
    "plt_metric_train(history=history6.history, metric=\"loss\", title=\"Train Loss\", x=1 ,y=0)\n",
    "\n",
    "\n",
    "# Plot the accuracy of contrastive loss\n",
    "plt_metric_val(history=history1.history, metric=\"accuracy\", title=\"Validation Accuracy\", x=0 ,y=1)\n",
    "\n",
    "# Plot the accuracy of dice loss\n",
    "plt_metric_val(history=history2.history, metric=\"accuracy\", title=\"Validation Accuracy\",x=0 ,y=1)\n",
    "\n",
    "# Plot the accuracy of BCE loss\n",
    "plt_metric_val(history=history3.history, metric=\"accuracy\", title=\"Validation Accuracy\",x=0 ,y=1)\n",
    "\n",
    "# Plot the accuracy of BCE dice loss\n",
    "plt_metric_val(history=history4.history, metric=\"accuracy\", title=\"Validation Accuracy\", x=0 ,y=1)\n",
    "\n",
    "# Plot the accuracy of hinge loss\n",
    "plt_metric_val(history=history5.history, metric=\"accuracy\", title=\"Validation Accuracy\", x=0 ,y=1)\n",
    "\n",
    "\n",
    "# Plot the contrastive loss\n",
    "plt_metric_val(history=history1.history, metric=\"loss\", title=\"Validation Loss\", x=1 ,y=1)\n",
    "\n",
    "# Plot the dice loss\n",
    "plt_metric_val(history=history2.history, metric=\"loss\", title=\"Validation Loss\", x=1 ,y=1)\n",
    "\n",
    "# Plot the BCE loss\n",
    "plt_metric_val(history=history3.history, metric=\"loss\", title=\"Validation Loss\", x=1 ,y=1)\n",
    "\n",
    "# Plot the BCE dice loss\n",
    "plt_metric_val(history=history4.history, metric=\"loss\", title=\"Validation Loss\", x=1 ,y=1)\n",
    "\n",
    "# Plot the hinge loss\n",
    "plt_metric_val(history=history5.history, metric=\"loss\", title=\"Validation Loss\", x=1 ,y=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRcXg3LLXdMn"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZ9TZ01LXdMo"
   },
   "outputs": [],
   "source": [
    "results1 = siamese1.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = siamese2.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = siamese3.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results4 = siamese4.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results5 = siamese5.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'CL': results1[0], 'DL': results2[0], 'BCEL': results3[0],\n",
    "        'BCDL': results4[0],'Hinge': results5[0]}\n",
    " \n",
    "courses = list(data.keys())\n",
    "values = list(data.values())\n",
    " \n",
    "fig = plt.figure(figsize=(15, 10))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values)\n",
    " \n",
    "plt.xlabel(\"Percent\")\n",
    "plt.ylabel(\"Different losses\")\n",
    "plt.title(\"Test loss\")\n",
    "plt.show()\n",
    "\n",
    "data = {'CL': results1[1], 'DL': results2[1], 'BCEL': results3[1],\n",
    "        'BCDL': results4[1],\n",
    "        'Hinge': results5[1]}\n",
    " \n",
    "courses = list(data.keys())\n",
    "values = list(data.values())\n",
    " \n",
    "fig = plt.figure(figsize=(15, 10))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values)\n",
    " \n",
    "plt.xlabel(\"Percent\")\n",
    "plt.ylabel(\"Different Losses\")\n",
    "plt.title(\"Test accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model (siamese1, show_shapes=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOdcBjXhXdMs"
   },
   "source": [
    "## Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jw0ysl9aXdMs"
   },
   "outputs": [],
   "source": [
    "predictions1 = siamese1.predict([x_test_1, x_test_2])\n",
    "visualize(pairs_test, labels_test, to_show=10, predictions=predictions1, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = siamese2.predict([x_test_1, x_test_2])\n",
    "visualize(pairs_test, labels_test, to_show=10, predictions=predictions2, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = siamese3.predict([x_test_1, x_test_2])\n",
    "visualize(pairs_test, labels_test, to_show=10, predictions=predictions3, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = siamese4.predict([x_test_1, x_test_2])\n",
    "visualize(pairs_test, labels_test, to_show=10, predictions=predictions4, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5 = siamese5.predict([x_test_1, x_test_2])\n",
    "visualize(pairs_test, labels_test, to_show=10, predictions=predictions5, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Possible Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy with balance factor (2023)\n",
    "Paper: \n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10021880 \n",
    "\n",
    "### Join of MSE, bidirectional Kullback Leibler divergence and rank order of quality scores (2023)\n",
    "Paper: \n",
    "https://dl.acm.org/doi/abs/10.1145/3597434\n",
    "\n",
    "### Multiple negative ranking loss (2022)\n",
    "Paper: \n",
    "https://arxiv.org/pdf/2203.14541.pdf\n",
    "\n",
    "### Max margin hinge loss (2021)\n",
    "Paper: \n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9414458\n",
    "\n",
    "### Softmax cross entropy loss (2018)\n",
    "Paper: \n",
    "https://www.researchgate.net/publication/326204812_Modeling_Contemporaneous_Basket_Sequences_with_Twin_Networks_for_Next-Item_Recommendation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "siamese_contrastive",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
