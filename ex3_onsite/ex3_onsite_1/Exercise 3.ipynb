{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Contents\n",
    "- Gradient Descent\n",
    "- Backpropagation\n",
    "- Backpropagation in PyTorch - Coding Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Cost function:** tells us how well did our network perform on examples given chosen weights and biases. \n",
    "- But how did that error come to be? Which weights (and biases) did we set correctly? Which ones not? What way do we need to alter them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Remember from calculus:** gradient of a function tells us direction of steepest ascent!\n",
    "- Therefore: moving in opposite direction (steepest descent) will minimize our function the most!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Goal of gradient descent:** decrease the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Update parameters ($\\theta$) in direction of steepest descent (will lead you to a local minimum)\n",
    "- **Gradient vector $\\nabla E(\\theta(k))$:** what direction do we adjust each parameter?\n",
    "- **Learning rate $\\tau$:** how big are our adjustments?\n",
    "\n",
    "**Anology**: walking down a mountain taking small steps into the direction of steepest descent instead of large ones until you reach a valley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Formula:** $ \\theta(k+1) = \\theta(k) - \\tau \\nabla E(\\theta(k)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For gradient descent we need thus need the partial derivative of our cost function with respect to **each weight and bias**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Gradient vector** will tell us **which direction to adjust each weight**, i.e. \"turn the knobs of our network\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Magnitude of each entry** in the gradient vector tells you how much it affected the outcome of the cost function (therefore, adjusting it in the direction of steepest descent (negative gradient) will have larger impact on reaching local minimum)\n",
    "    - For example: z = 2x + y\n",
    "    - $ \\frac{dz}{dx} = 2 $ and $ \\frac{dz}{dy} = 1 $, i.e. x contributes twice as much as y to the outcome of z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Computing the gradient of a deeply nested function**\n",
    "- **Essentially:** applying chain rule over and over again\n",
    "- Chain rule: $E = f(g(x)) \\rightarrow \\nabla E(x) = \\nabla g(x) * \\nabla f(g(x))$\n",
    "\n",
    "**Unravels our deeply nested functions in a backward manner!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Important observation:** During gradient calculation in a compute graph, each node is just interested in its own input and output.\n",
    "- Only need to compute local gradient with respect to its inputs\n",
    "- Then multiplied with gradient that has been backpropagated to this node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"backprop_idea.jpg\" alt=\"Drawing\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What if we are dealing with matrices? Same thing, gradients become matrices (transposed jacobians).\n",
    "- **Note:** Important to keep the order on the backward pass! $A @ B \\neq  B @ A$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"backprop_idea_matrix.jpg\" alt=\"Drawing\" style=\"width: 60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a lot to digest! Take your time and practice your skills by drawing simple compute graphs and calculating the partial derivatives.\n",
    "\n",
    "I recommend the videos of 3Blue1Brown on this topic:\n",
    "- https://youtu.be/IHZwWFHWa-w\n",
    "- https://youtu.be/Ilg3gGewQ5U\n",
    "- https://youtu.be/tIeHLnjs5U8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation in PyTorch - Coding Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Tensors**: data structure used in PyTorch (uses taping for remembering computations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Attributes:**\n",
    "    - ```data```: our data stored in the Tensor (e.g. input features) \n",
    "    - ```requires_grad```: want to compute gradient with respect to Tensor at some point. \n",
    "    - ```grad```: If ```requires_grad``` is ```True``` initialised as zero-array with shape of ```data``` (every entry has a gradient associated with it).\n",
    "    - ```grad_fn```: Function which created the tensor (e.g. result of an addition).\n",
    "    - ```num_forward```: tracks how many (forward) functions were applied on top of the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Functions:**\n",
    "    - ```backward```: how is the backward pass computed for this Tensor? Essentially two steps: add up incoming gradient to gradient to gradient of Tensor and call backward on gradient function (continue the backward pass and send along local gradient)\n",
    "    - ```zero_grad```: resets the gradients of the Tensor to all zeros again\n",
    "    - ```__add__```: the sample operation we are overwriting in this exercise (calls the Add() function's forward function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "    def __init__(self, data, requires_grad=False):\n",
    "        self.data = np.array(data)\n",
    "        self.requires_grad = requires_grad\n",
    "        if self.requires_grad:\n",
    "            self.grad = np.zeros_like(self.data)\n",
    "        self.grad_fn = None\n",
    "        self.num_forward = -1\n",
    "    \n",
    "    def backward(self, dLdt=1.):\n",
    "        if self.requires_grad:\n",
    "            self.grad += dLdt\n",
    "            if self.num_forward > 0:\n",
    "                self.num_forward -= 1\n",
    "            elif self.grad_fn is not None:\n",
    "                self.grad_fn.backward(self.grad)\n",
    "                del self.grad_fn\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Add().forward(self, other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the exercise you will implement child classes of the parent class `Function()` (resembles basic mathmatical computations that can be performed on `Tensors()`)\n",
    "- **Functions:**\n",
    "    - `forward()`: Given an input, how is the mathmatical operation performed and what is the output?\n",
    "    - `backward()`: Given an incoming gradient, how is the local gradient computed and propagated backwards to each input parameter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Function():\n",
    "    def forward(self, *tensors):\n",
    "        for t in tensors:\n",
    "            t.num_forward += 1\n",
    "    \n",
    "    def backward(self, dLdf):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Addition Example:**\n",
    "\n",
    "- `forward()`: given input tensor `t1` and `t2`:\n",
    "    - save references to tensors involved in computation\n",
    "    - output is new tensor with  `data = t1.data + t2.data`\n",
    "    - if at least one input requires gradient, `requires_grad = True` of output tensor\n",
    "    - `grad_fn` of output is set to `Add()` (i.e. `self`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `backward()`: given incoming gradient `dLdf`:\n",
    "    - Compute local gradient with respect to each input (`t1` and `t2`)\n",
    "    - **Idea:** send local gradient of input + incoming gradient backwards to respective input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    def forward(self, t1, t2):\n",
    "        super().forward(t1, t2)\n",
    "        self.t1 = t1\n",
    "        self.t2 = t2\n",
    "        requires_grad = t1.requires_grad or t2.requires_grad\n",
    "        t = Tensor(t1.data+t2.data, requires_grad=requires_grad)\n",
    "        t.grad_fn = self\n",
    "        return t\n",
    "    \n",
    "    def backward(self, dLdf):\n",
    "        # gradient with respect to t1\n",
    "        dfdt1 = 1\n",
    "        grad_t1 = dfdt1 * dLdf\n",
    "        self.t1.backward(grad_t1)\n",
    "        \n",
    "        # gradient with respect to t2\n",
    "        dfdt2 = 1\n",
    "        grad_t2 = dfdt2 * dLdf\n",
    "        self.t2.backward(grad_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise:** what are the partial derivatives with respect to each parameter for this computation graph?\n",
    "\n",
    "$$ a + b = c $$\n",
    "\n",
    "$$ a + c = d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{dd}{dd} = \\text{  ?  ;}  \\frac{dd}{dc} = \\text{  ?  ;}  \\frac{dd}{db} = \\text{  ?  ;}  \\frac{dd}{da} = \\text{  ?  } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"compute_graph.png\" alt=\"Drawing\" style=\"width: 30%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution using backpropagation:**\n",
    "- Start at end of compute graph, i.e. at $d$. \n",
    "    - Compute the local derivatives with respect to the inputs (i.e. $c$ and $a$)\n",
    "        - Thus compute: $\\frac{dd}{dc} = 1$ and $\\frac{dd}{da} = 1$\n",
    "    - Since there was no incoming gradient, we default it to 1\n",
    "        - Equivalent to $\\frac{dd}{dd} = 1$\n",
    "- From $d$ send back to each input the local gradient with respect to said input times the incoming gradient\n",
    "     - Send back to $c$: $\\frac{dd}{dc} * 1$\n",
    "     - Send back to $a$: $\\frac{dd}{da} * 1$\n",
    "- At $c$: \n",
    "    - Add up the incoming gradients towards the gradient of $c$ (thus $\\frac{dd}{db} = 1$)\n",
    "    - Compute the local derivatives with respect to the inputs (i.e. $a$ and $b$).\n",
    "        - Thus compute: $\\frac{dc}{da} = 1$ and $\\frac{dc}{db} = 1$\n",
    "    - The incoming gradient from $d$ was 1\n",
    "- From $c$:\n",
    "    - Send back to a: $\\frac{dc}{da} * 1$\n",
    "    - Send back to b: $\\frac{dc}{db} * 1$\n",
    "- At $b$:\n",
    "    - Add up the incoming gradients towards the gradient of $c$ (thus $\\frac{dd}{db} = 1$)\n",
    "    - Node has no inputs; backpropagation finishes\n",
    "- At $a$:\n",
    "    - Add up the incoming gradients towards the gradient of $a$ \n",
    "        - $a$ has two incoming gradients (from $d$ and $c$) (thus $\\frac{dd}{da} = 1 + 1$)\n",
    "    - Node has no inputs; backpropagation finishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Or directly written out using chain rule:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{dd}{dd} = 1 $$\n",
    "\n",
    "$$ \\frac{dd}{dc} = 1$$\n",
    "\n",
    "$$ \\frac{dd}{db} = \\frac{dd}{dc} * \\frac{dc}{db} = 1 * 1 = 1 $$\n",
    "\n",
    "$$ \\frac{dd}{da} = \\frac{dd}{da} + \\frac{dd}{dc} * \\frac{dc}{da}= 1 + 1 * 1 = 2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Calling backward on d will now compute the gradient along the computation graph for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "d.backward()\n",
    "\n",
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** can we call `backward()` again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'grad_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1z/t2zg4rcj06q2pyswk5wm8wl00000gn/T/ipykernel_66011/608206150.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/1z/t2zg4rcj06q2pyswk5wm8wl00000gn/T/ipykernel_66011/1927484246.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dLdt)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_forward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_forward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'grad_fn'"
     ]
    }
   ],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** no, computation graph is deleted after the first gradient backpropagation as most of the times there is no use in keeping this graph alive. Therefore a new computation graph is built for every forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question:** what happens if we run the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** gradient accumulates! That is why we need to call `zero_grad()`to reset the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
