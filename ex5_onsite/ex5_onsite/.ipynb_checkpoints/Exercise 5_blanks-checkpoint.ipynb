{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b121cdd",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3937d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d26877",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- Custom DataLoaders\n",
    "- Learning Rate Schedules\n",
    "- Weight Initalizations\n",
    "- BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5297f",
   "metadata": {},
   "source": [
    "## Custom DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4651562",
   "metadata": {},
   "source": [
    "### Task 1: Defining your own DataLoader with z-score normalisation\n",
    "- Define a new Class `WineDataset` which inherits from class `torch.utils.data.Dataset` and load the `wine.csv` file correctly. To do so, overwrite the `__init__`, `__len__` and `__getitem__` function of the class you created\n",
    "- Within the `__init__` function make it so that your data is z-score normalised, i.e. each record is zero-centered and normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab37c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self, path, range_data):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "    \n",
    "training_set = WineDataset('wine.csv', range(140))\n",
    "validation_set = WineDataset('wine.csv', range(140,170))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709b9e84",
   "metadata": {},
   "source": [
    "### Take-Home Assignment 1\n",
    "- Write a similar DataLoader for the Wine Quality dataset from Exercise 1. Apply now z-score normalization per column(!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876780e",
   "metadata": {},
   "source": [
    "### Task 2: Learning Rate Schedules\n",
    "\n",
    "- Extend the train loop as you know it from previous exercises such that it lowers the learning rate after 10 epochs by a factor of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f59067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassificationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(13, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1.forward(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_loop_schedule(train_loader, val_loader, network, loss_fun, optimizer, epochs, print_freq=1):\n",
    "    for e in range(epochs):\n",
    "        train_loss = 0.\n",
    "\n",
    "        # Training\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.float(), y.long()\n",
    "            \n",
    "            # Prediction\n",
    "            y_pred = network.forward(x)\n",
    "            batch_loss = loss_fun(y_pred, y)\n",
    "            train_loss += batch_loss\n",
    "            \n",
    "            # Optimization\n",
    "            network.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Inference\n",
    "        val_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(val_loader):\n",
    "                x, y = x.float(), y.long()\n",
    "                y_pred = network.forward(x)\n",
    "                val_loss += loss_fun(y_pred, y)\n",
    "        \n",
    "        # Print epoch results only every print_freq iterations\n",
    "        if (e + 1) % print_freq == 0:\n",
    "            print(\"Epoch: {}/{}; Training loss: {}; Validation loss {}\"\n",
    "                  .format(e+1, epochs, train_loss / len(train_loader), val_loss / len(val_loader)))\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d780e6c",
   "metadata": {},
   "source": [
    "Test your implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f00995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "lr = 0.1\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "network = LinearClassificationNet()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "train_loop_schedule(train_loader, val_loader, network, loss_fun, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036bc055",
   "metadata": {},
   "source": [
    "### Take-Home Assignment 2\n",
    "\n",
    "- Now, extend the train loop such that it lowers the learning rate if the network has not improved in 5 epochs. To do so you need to track the current best loss that you have achieved so far as well as during which epoch said loss occured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb254cee",
   "metadata": {},
   "source": [
    "## Weight Initalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e0059",
   "metadata": {},
   "source": [
    "During lecture you saw undesirable property of networks, namely activations either drift apart or converge to zero the deeper we get in our network. This is undesireable as it complicates training. In general we want that activations live in the same distribution across layers. Will make our training more efficient. To demontrate you this consider the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5deeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(10, 100, bias=False),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100, 100, bias=False),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100, 100, bias=False),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100, 1, bias=False),\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def track_activations(activations):\n",
    "    def hook_fun(module, input, output):\n",
    "        activations.append(output.detach())\n",
    "    return hook_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "net = Network()\n",
    "\n",
    "hooks = []\n",
    "activations = []\n",
    "hook_fun = track_activations(activations)\n",
    "\n",
    "for layer in net.layers:\n",
    "    hook = layer.register_forward_hook(hook_fun)\n",
    "    hooks.append(hook)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = net(x)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, act in enumerate(activations):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.hist(act.reshape(-1).numpy(), bins=9, density=True, range=(-2.1,2.1))\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabd81b",
   "metadata": {},
   "source": [
    "As you can see the activations get smaller and smaller the deeper we go in our network. To dampen this effect we can use weight initalization schemes. You got to know Kaiming He's initialization scheme. Using this scheme we make it so that activations across layers are approximately distributed in the same normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d291a",
   "metadata": {},
   "source": [
    "### Task 3: Using Weight Initalization\n",
    "\n",
    "Now, initalize the weights of your network before starting training! Use Kaiming He's initialisation which you know from the lecture (see https://pytorch.org/docs/stable/nn.init.html). Write the `init_weight` function which iteratively checks each module and initalizes the weights if the module is of type `nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    pass\n",
    "        \n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []\n",
    "activations = []\n",
    "hook_fun = track_activations(activations)\n",
    "\n",
    "for layer in net.layers:\n",
    "    hook = layer.register_forward_hook(hook_fun)\n",
    "    hooks.append(hook)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = net(x)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, act in enumerate(activations):\n",
    "    plt.subplot(4, 2, i+1)\n",
    "    plt.hist(act.reshape(-1).numpy(), bins=9, density=True, range=(-2.1,2.1))\n",
    "    plt.gca().set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe12a8",
   "metadata": {},
   "source": [
    "## BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e9f1e",
   "metadata": {},
   "source": [
    "Batch Normalization is another way to ensure that activations across layers stay normally distributed. It more aggressive as in that it intermediatly normalizes the data again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcdafd",
   "metadata": {},
   "source": [
    "### Task 4: Using BatchNorm Layers\n",
    "\n",
    "- Extend the network below by adding BatchNorm layers after each Linear layer. Use PyTorch's `BatchNorm1D` class (https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) to do so. See how the BatchNorm layer changes your data along the flow of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(10, 100, bias=False),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100, 100, bias=False),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100, 100, bias=False),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100, 1, bias=False),\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "net = Network()\n",
    "\n",
    "hooks = []\n",
    "activations = []\n",
    "hook_fun = track_activations(activations)\n",
    "\n",
    "for layer in net.layers:\n",
    "    hook = layer.register_forward_hook(hook_fun)\n",
    "    hooks.append(hook)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = net(x)\n",
    "\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, act in enumerate(activations):\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(act.reshape(-1).numpy(), bins=9, density=True, range=(-2.1,2.1))\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd65235",
   "metadata": {},
   "source": [
    "## Pros and Cons for both methods:\n",
    "- **Weight Initalization:**\n",
    "    - Pro: Minimal computation overhead; Con: Effect wears off the deeper you go\n",
    "- **BatchNorm:**\n",
    "    - Pro: Can be applied even at deep layers; Con: Expensive operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e735db3",
   "metadata": {},
   "source": [
    "### Take-Home Assignment 2:\n",
    "- Try incorporating BatchNorm layers and different weight initialisation in your MNIST training code. Play around with deep networks with and without said techniques and exhibit how the loss changes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
