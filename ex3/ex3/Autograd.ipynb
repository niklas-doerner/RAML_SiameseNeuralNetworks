{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we have already discussed the gradient backpropagation algorithm for calculating derivatives in neural networks. In this exercise you will implement this algorithm in a similar fashion as it is done in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import nn\n",
    "import toolbox as tb\n",
    "from toolbox import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the general idea, run the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand what's going on here. After we have run ``d.backward()`` the variables ``a``, ``b`` and ``c`` involved in the computation of ``d`` now feature an attribute ``grad``. This attribute represents the gradient of each variable w.r.t. computing ``d``:\n",
    "$$\n",
    "\\frac{\\mathrm d \\mathop{}\\! d}{\\mathrm d \\mathop{}\\! c} = \\frac{\\mathrm d \\mathop{}\\! c}{\\mathrm d \\mathop{}\\! c} + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! c} = 1 + 0 = 1, \\\\\n",
    "\\frac{\\mathrm d \\mathop{}\\! d}{\\mathrm d \\mathop{}\\! b} = \\frac{\\mathrm d \\mathop{}\\! c}{\\mathrm d \\mathop{}\\! b} + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! b} = 1 + 0 = 1, \\\\\n",
    "\\frac{\\mathrm d \\mathop{}\\! d}{\\mathrm d \\mathop{}\\! a} = \\frac{\\mathrm d \\mathop{}\\! c}{\\mathrm d \\mathop{}\\! a} + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! a} = \\left(\\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! a} + \\frac{\\mathrm d \\mathop{}\\! b}{\\mathrm d \\mathop{}\\! a}\\right) + \\frac{\\mathrm d \\mathop{}\\! a}{\\mathrm d \\mathop{}\\! a} = \\left( 1 + 0 \\right) + 1 = 2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does ``a`` know about the computations going on after its creation? The answer is that every operation we apply to tensors creates a new object representing this operation with all its variables involved. The output tensor of these operations saves a reference to the the operation which created it in the attribute ``grad_fn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<toolbox.Add object at 0x7f3370693a90>\n",
      "<toolbox.Add object at 0x7f3370693af0>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "print(d.grad_fn)\n",
    "print(c.grad_fn)\n",
    "print(b.grad_fn)\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above you can see that ``c`` and ``d`` resulted from addition operations. Variables ``a`` and ``b`` were created from scratch and therfore have the value ``None`` as ``grad_fn``. This mechnanism implicitly builds a computation graph with all the dependencies among the variables. The function call ``d.backward()`` is a shorthand for ``d.backward(1.)`` and starts the gradient backpropagation involving all variables involved in the creation of ``d``. The only exception are tensors for which the ``requires_grad`` evaluates to ``False``, which is also the default value for the creation of a tensor object.\n",
    "\n",
    "Let's try to call the ``backward`` function two times in a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'grad_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m d \u001b[38;5;241m=\u001b[39m c \u001b[38;5;241m+\u001b[39m a\n\u001b[1;32m      7\u001b[0m d\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m----> 8\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/University/Master/Deep Learning/Ãœbungen/ex3/ex3/toolbox.py:19\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, dLdt)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_forward \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_forward \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_fn\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_fn\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'grad_fn'"
     ]
    }
   ],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is thrown, because the computation graph is deleted after the first gradient backpropagation as most of the times there is no use in keeping this graph alive. Therefore a new computation graph is built for every forward pass.\n",
    "\n",
    "Also observe the behaviour of the gradient calculation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are accumulated over time and hence the values above are incorrect. Thus it will be important to reset the gradient values for parameters we want to optimize over to zero after each forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "a = Tensor(1., requires_grad=True)\n",
    "b = Tensor(2., requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "a.zero_grad()\n",
    "b.zero_grad()\n",
    "\n",
    "c = a + b\n",
    "d = c + a\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print(d.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the file ``toolbox.py`` and make sure you understand the basic mechanism of how it implements the creation of computation graphs. Then follow the comments and implement the backward pass for ``Mul``, ``Div``, ``Pow``, ``MatMul``, ``ReLU``, ``Exp``, ``Log``, ``Sum``, ``Mean`` and ``GetItem``. The addition function ``Add`` is already implemented and may serve as a reference. Test you implementations by running the cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mul [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(2., requires_grad=True)\n",
    "b = Tensor(3., requires_grad=True)\n",
    "\n",
    "c = a * b\n",
    "c.backward(2.)\n",
    "\n",
    "assert np.abs(a.grad-6.)<1e-6 and np.abs(b.grad-4.)<1e-6, \"Multiplication doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Div [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([1., 2.]), requires_grad=True)\n",
    "b = Tensor(np.array([3., 4.]), requires_grad=True)\n",
    "\n",
    "c = a / b\n",
    "c.backward(0.5)\n",
    "\n",
    "assert np.linalg.norm(a.grad-np.array([1./6, 1./8]))<1e-6 and \\\n",
    "np.linalg.norm(b.grad-np.array([-1./18, -1./16]))<1e-6, \\\n",
    "\"Division doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pow [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A ** 4\n",
    "b.backward(np.array([[-1., 0], [1., 2.]]))\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([[-4., 0.], [108., 512.]]))<1e-6, \"Exponentiation doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatMul [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "b = Tensor(np.array([[1.], [2.]]), requires_grad=True)\n",
    "c = Tensor(np.array([[3., 4.]]), requires_grad=True)\n",
    "\n",
    "d = c @ A @ b\n",
    "d.backward(-1.5)\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([[-4.5, -9.], [-6., -12.]]))<1e-6 and \\\n",
    "np.linalg.norm(b.grad-np.array([[-22.5], [-33.]]))<1e-6 and \\\n",
    "np.linalg.norm(c.grad-np.array([[-7.5, -16.5]]))<1e-6, \"Matrix multiplication doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "ReLU doesn't work properly!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m tb\u001b[38;5;241m.\u001b[39mrelu(A)\n\u001b[1;32m      4\u001b[0m b\u001b[38;5;241m.\u001b[39mbackward(np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m4.\u001b[39m, \u001b[38;5;241m3.\u001b[39m], [\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m1.\u001b[39m], [\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m]]))\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(A\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m], [\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m1.\u001b[39m], [\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m]]))\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReLU doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work properly!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ReLU doesn't work properly!"
     ]
    }
   ],
   "source": [
    "A = Tensor(np.array([[-1., -2,], [0.5, 2.], [1., 5.]]), requires_grad=True)\n",
    "\n",
    "b = tb.relu(A)\n",
    "b.backward(np.array([[4., 3.], [2., 1.], [0., -1.]]))\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([[0., 0.], [2., 1.], [0., -1.]]))<1e-6, \"ReLU doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[-1.], [2.]]), requires_grad=True)\n",
    "\n",
    "b = tb.exp(A)\n",
    "b.backward(np.array([[-1.], [2.]]))\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.exp(A.data)*np.array([[-1.], [2.]]))<1e-6, \\\n",
    "\"The exponential function doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([0.1, 1., 2., np.exp(100.)]), requires_grad=True)\n",
    "\n",
    "b = tb.log(A)\n",
    "b.backward(1.5)\n",
    "\n",
    "assert np.linalg.norm(A.grad-np.array([15., 1.5, 0.75, 1.5*np.exp(-100.)]))<1e-6, \"Logarithm doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Summation doesn't work properly!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      5\u001b[0m b\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m2.5\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(A\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.5\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummation doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt work properly!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Summation doesn't work properly!"
     ]
    }
   ],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A.sum(dim=1)\n",
    "b = b.sum()\n",
    "b.backward(2.5)\n",
    "\n",
    "assert np.linalg.norm(A.grad-2.5*np.ones((2, 2)))<1e-6, \"Summation doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A.mean()\n",
    "b.backward()\n",
    "\n",
    "assert np.linalg.norm(A.grad-0.25*np.ones((2, 2)))<1e-6, \"Mean doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GetItem [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Tensor(np.array([[1., 2.], [3., 4.]]), requires_grad=True)\n",
    "\n",
    "b = A[0, 1]\n",
    "b.backward(5.)\n",
    "\n",
    "assert np.linalg.norm(A.grad-5.*np.array([[0., 1.], [0., 0.]]))<1e-6, \"Indexing doesn't work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above tests were successful, you should be able to backpropagate through a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Something is wrong...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m l \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mforward(network\u001b[38;5;241m.\u001b[39mforward(data), target)\n\u001b[1;32m     14\u001b[0m l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(network\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m4.\u001b[39m], [\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m4.\u001b[39m]]))\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-6\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     17\u001b[0m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(network\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mb\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m2.\u001b[39m], [\u001b[38;5;241m2.\u001b[39m]]))\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-6\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     18\u001b[0m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(network\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[ \u001b[38;5;241m8.00168889e-34\u001b[39m,  \u001b[38;5;241m1.73369926e-33\u001b[39m],\n\u001b[1;32m     19\u001b[0m                                                   [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6.00000000e+00\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.30000000e+01\u001b[39m],\n\u001b[1;32m     20\u001b[0m                                                   [ \u001b[38;5;241m6.00000000e+00\u001b[39m,  \u001b[38;5;241m1.30000000e+01\u001b[39m]]))\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-6\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m     21\u001b[0m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(network\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mb\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[ \u001b[38;5;241m1.33361482e-34\u001b[39m],\n\u001b[1;32m     22\u001b[0m                                                   [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.00000000e+00\u001b[39m],\n\u001b[1;32m     23\u001b[0m                                                   [ \u001b[38;5;241m1.00000000e+00\u001b[39m]]))\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1e-6\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSomething is wrong...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Something is wrong..."
     ]
    }
   ],
   "source": [
    "network = nn.Network()\n",
    "network.add_layer(nn.LinearLayer(np.array([[1., 2.], [3., 4.]]), \\\n",
    "                                np.array([[1.], [2.]])))\n",
    "network.add_layer(nn.ReLU())\n",
    "network.add_layer(nn.LinearLayer(np.array([[1., 2.], [3., 4.], [5., 6.]]), \\\n",
    "                 np.array([[1.], [2.], [3.]])))\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "data = Tensor(np.array([[1.], [2.]]))\n",
    "target = 1\n",
    "\n",
    "l = loss.forward(network.forward(data), target)\n",
    "l.backward()\n",
    "\n",
    "assert (np.linalg.norm(network.layers[0].W.grad-np.array([[2., 4.], [2., 4.]]))<1e-6 and\n",
    "np.linalg.norm(network.layers[0].b.grad-np.array([[2.], [2.]]))<1e-6 and\n",
    "np.linalg.norm(network.layers[2].W.grad-np.array([[ 8.00168889e-34,  1.73369926e-33],\n",
    "                                                  [-6.00000000e+00, -1.30000000e+01],\n",
    "                                                  [ 6.00000000e+00,  1.30000000e+01]]))<1e-6 and\n",
    "np.linalg.norm(network.layers[2].b.grad-np.array([[ 1.33361482e-34],\n",
    "                                                  [-1.00000000e+00],\n",
    "                                                  [ 1.00000000e+00]]))<1e-6), \"Something is wrong...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
